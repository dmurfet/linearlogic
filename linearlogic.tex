\documentclass[english,letter paper,12pt,reqno]{article}
\usepackage{etex} % to fix "no room for new dimen" error. Voodoo.
\usepackage{array}
\usepackage{bussproofs}
\usepackage{epigraph}
\usepackage{stmaryrd}
\usepackage{mathdots}
\usepackage{amsmath, amscd, amssymb, mathrsfs, accents, amsfonts,amsthm}
\usepackage[all]{xy}
\usepackage{mathtools} % for bra-ket
\usepackage{tikz}
\usetikzlibrary{calc}

\AtEndDocument{\bigskip{\footnotesize%
  \textsc{Department of Mathematics, University of Southern California} \par  
  \textit{E-mail address}: \texttt{murfet@usc.edu} \par
}}

% Labels in tabular
\newcommand{\tagarray}{\mbox{}\refstepcounter{equation}$(\theequation)$}

% Bra-ket stuff
\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1 \delimsize\vert #2}
\DeclarePairedDelimiterX\inner[2]{\langle}{\rangle}{#1,#2}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\set{\lbrace}{\rbrace}

%\begin{align*}
%\bra{a}       &= \bra*{\frac{a}{1}}\    \ket{a}       &= \ket*{\frac{a}{1}}\    \braket{a}{b} &= \braket*{\frac{a}{1}}{\frac{b}{1}}\    %\inner{a}{b}  &= \inner*{\frac{a}{1}}{\frac{b}{1}}\    \abs{a}       &= \abs*{\frac{a}{1}}\    \norm{a}      &= \norm*{\frac{a}%{1}}\    \set{a,b}     &= \set*{\frac{a}{1},\frac{b}{1}}
%\end{align*}

% TikZ stuff
\def\drawbang{\draw[color=teal!50, line width=2pt]}
\def\drawprom{\draw[color=gray, line width=3pt]}
\def\bluenode{\node[circle,draw=blue!50,fill=blue!20]}
\def\mapnode{\node[circle,draw=black,fill=black,inner sep=0.5mm]}
\def\whitenode{\node[circle,draw=blue!50,fill=blue!5]}
\def\dernode{\node[circle,draw=black,fill=white]}
\definecolor{Myblue}{rgb}{0,0,0.6}
\usepackage[a4paper,colorlinks,citecolor=Myblue,linkcolor=Myblue,urlcolor=Myblue,pdfpagemode=None]{hyperref}

\SelectTips{cm}{}

\setlength{\evensidemargin}{0.1in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\textwidth}{6.3in}
\setlength{\topmargin}{0.0in}
\setlength{\textheight}{8.5in}
\setlength{\headheight}{0in}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newtheoremstyle{example}{\topsep}{\topsep}
	{}
	{}
	{\bfseries}
	{.}
	{2pt}
	{\thmname{#1}\thmnumber{ #2}\thmnote{ #3}}
	
	\theoremstyle{example}
	\newtheorem{definition}[theorem]{Definition}
	\newtheorem{example}[theorem]{Example}
	\newtheorem{remark}[theorem]{Remark}
	\newtheorem{question}[theorem]{Question}

\numberwithin{equation}{section}

% Operators
\def\eval{\operatorname{ev}}
\def\sh{\operatorname{Sh}}
\def\res{\operatorname{Res}}
\def\Coker{\operatorname{Coker}}
\def\Ker{\operatorname{Ker}}
\def\im{\operatorname{Im}}
\def\can{\operatorname{can}}
\def\K{\mathbf{K}}
\def\D{\mathbf{D}}
\def\N{\mathbf{N}}
\def\LG{\mathcal{LG}}
\def\Ab{\operatorname{Ab}}
\def\Hom{\operatorname{Hom}}
\def\modd{\operatorname{mod}}
\def\Modd{\operatorname{Mod}}
\DeclareMathOperator{\Ext}{Ext}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\tot}{Tot}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\str}{str}
\DeclareMathOperator{\hmf}{hmf}
\DeclareMathOperator{\HMF}{HMF}
\DeclareMathOperator{\hf}{HF}
\DeclareMathOperator{\At}{At}
\DeclareMathOperator{\Cat}{Cat}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\MSpec}{MSpec}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\LC}{LC}
\def\inta{\bold{int}}
\def\comp{\underline{\textup{comp}}}
\def\contract{\;\lrcorner\;}

% SCprooftree environment
\newenvironment{scprooftree}[1]
{\gdef\scalefactor{#1}\begin{center}\proofSkipAmount \leavevmode}
{\scalebox{\scalefactor}{\DisplayProof}\proofSkipAmount \end{center} }
  
\begin{document}

% Bussproof things
\def\ScoreOverhang{1pt}

% Commands
\def\Res{\res\!}
\newcommand{\ud}[1]{\operatorname{d}\!{#1}}
\newcommand{\Ress}[1]{\res_{#1}\!}
\newcommand{\cat}[1]{\mathcal{#1}}
\newcommand{\lto}{\longrightarrow}
\newcommand{\xlto}[1]{\stackrel{#1}\lto}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\md}[1]{\mathscr{#1}}
\newcommand{\church}[1]{\underline{#1}}
\newcommand{\prf}[1]{\underline{#1}}
\newcommand{\den}[1]{\llbracket #1 \rrbracket}
\def\l{\,|\,}
\def\sgn{\textup{sgn}}
\def\cont{\operatorname{cont}}

\title{Logic and linear algebra: an introduction}
\author{Daniel Murfet}

\maketitle

\begin{abstract} We give an introduction to logic tailored for algebraists, by explaining how to represent proofs in linear logic as linear maps between vector spaces. The interesting part of this vector space semantics is based on the cofree cocommutative coalgebra of Sweedler \cite{sweedler} and the recent explicit computations of liftings in \cite{murfet_coalg}.
\end{abstract}

\setlength{\epigraphwidth}{0.6\textwidth}
\epigraph{\emph{A contrario}, for intuitionists, \emph{Modus Ponens} is not a legal advisor, it is the door open on a new world, it is the application of a function $(A \Rightarrow B)$ to an argument $(A)$ yielding a result $(B)$. Proofs are no longer those sequences of symbols created by a crazy bureaucrat, they are functions, \emph{morphisms}.}{Jean-Yves Girard, \textit{The Blind Spot}}

\section{Introduction}

%- Logic is about structure of sets of proofs
%- Convince them there is is structure
%- Complexity is about interaction between proofs and cut-elimination

Logic is familiar to mathematicians in other fields primarily as the study of the \emph{truth} of mathematical propositions, but there is an alternative tradition which views logic as being about the structure of the \emph{collection of proofs}. The contrast between these points of view can be explained by an analogy (not perfect, but perhaps helpful) between propositions in logic and differential equations. In this analogy the truth of a proposition corresponds to the \emph{existence} of a solution to a differential equation, and the set of proofs of a proposition plays the role of the space of solutions. There is often a great deal of redundancy in the space of solutions, in the sense that two solutions which are apparently different may be related by a symmetry of the underlying manifold, and thus are in essence ``the same''. Moreover there are examples, such as the Seiberg-Witten equations, where the moduli space obtained by identifying solutions related by symmetries is a compact manifold, whose ``finite'' geometry is the key to understanding the content of the original equations.

There was an analogous phenomenon at the founding of formal symbolic logic, when a central problem was to establish the consistency of arithmetic. While working on this problem Gentzen discovered a new style of presenting proofs, called the \emph{sequent calculus}, in which the set of proofs of any arithmetic proposition is revealed to contain a great deal of redundancy: many apparently different proofs actually have ``the same'' logical content. This redundancy has its origin in applications of \emph{Modus Ponens}, which is known in sequent calculus as the \emph{cut rule}. This rule creates indirection and implicitness in proofs, by making theorems depend on lemmas (whose proofs are somewhere else, hence the indirection). However this implicitness can be ``explicated'' without essentially changing the content of the proof; this deep result is known as Gentzen's \emph{Hauptsatz}. Identifying proofs related by this explicitation (called \emph{cut-elimination}) yields the much more tractable set of \emph{cut-free proofs}, the analogue of the compact manifold of solutions modulo symmetry. By reducing consistency to a problem about cut-free proofs, where it is trivial, Gentzen was able to prove the consistency of arithmetic. For the full details of this story see \cite{??,??}.

\vspace{0.5cm}

%It is still unclear how seriously to take the analogy between explication of proofs and the action on solutions by symmetries of an underlying manifold, but the comparision is not an idle one, as we will explain. 
  % Algebra and logic are connected in various ways, for example through categorical logic and coherence theorems \cite{??}, topos theory and categories of sheaves \cite{??} and through model theory and its applications in algebraic geometry \cite{??}. Part of the purpose of this note is to provide the necessary background for connections between linear logic, higher categories and algebraic geometry \cite{??} (see Section \ref{section:sketch} of this introduction). 
  
The purpose of this article is to introduce the reader to this alternative tradition of logic, with its emphasis on the structure and symmetries of collections of proofs. We will do this using linear logic and its semantics in vector spaces and linear maps. Here the word ``semantics'' has a meaning close to what we mean by \emph{representation} in algebra: the structure of linear logic is encoded in its connectives, deduction rules, and cut-elimination transformations, and some insight into this structure can be gained by mapping it in a structure-preserving way to linear operators on vector spaces.

The outline of the article is as follows: in Section \ref{section:logic} we introduce the $\lambda$-calculus, intuitionistic logic and linear logic, with an emphasis on the role of duplication. In Section \ref{section:diagrammatics} we assign to every proof in intuitionistic linear logic a string diagram and corresponding linear map, and give a detailed example. In Section \ref{section:cut_elim} we turn to cut-elimination, examining a mildly non-trivial example in detail. In Appendix \ref{section:example_lifting} we examine tangent maps in connection with proofs in linear logic. Most of what we have to say is well-known, with the exception of some aspects of the vector space semantics in Section \ref{section:vector_space_sem}. There are many aspects of logic and its connections with other subjects that we cannot cover: for a well-written account of analogies between logic, topology and physics we recommend the survey of Baez and Stay \cite{baez}.
\\

\emph{Acknowledgements.} Thanks to Nils Carqueville, Jesse Burke, and Andante.
\\

\section{A sketch of linear logic as a language}\label{section:sketch}

Linear logic was introduced by Girard in the 1980s and has been the subject of active study ever since. Its special features make it naturally connected with algebra (broadly interpreted). At root this connection is linguistic: symmetric closed monoidal categories are ubiquitous in algebra, and their formal language is a subset of linear logic. For example, writing $(-) \multimap (-)$ for the internal Hom in a symmetric closed monoidal category $\cat{C}$, there is for any triple of objects $a,b,c \in \cat{C}$ a canonical map
\begin{equation}\label{eq:comp_map_intro}
(a \multimap b) \otimes (b \multimap c) \lto a \multimap c
\end{equation}
which is the internal notion of \emph{composition}. It is derived from the structure of the category $\cat{C}$ in the following way: from the evaluation maps
\[
e_{a,b}: a \otimes (a \multimap b) \lto b, \qquad e_{b,c}: b \otimes (b \multimap c) \lto c
\]
and the adjunction between internal Hom and tensor we obtain a map
\begin{equation}
\xymatrix{
\Hom_{\cat{C}}( c, c ) \ar[d]^{ \Hom( e_{b,c}, 1 ) }\\
\Hom_{\cat{C}}( b \otimes (b \multimap c), c) \ar[d]^{ \Hom(e_{a,b} \otimes 1,1) }\\
\Hom_{\cat{C}}(a \otimes (a \multimap b) \otimes (b \multimap c),  c ) \ar[d]^{\cong \quad \textup{adjunction}}\\
\Hom_{\cat{C}}((a \multimap b) \otimes (b \multimap c), a \multimap c )\,.
} \label{eq:construct_comp_catt}
\end{equation}
The image of the identity on $c$ under this map is the desired composition. 

This construction is formal, in the sense that it does not depend on the nature of the particular objects $a,b,c$. The formality can be made precise by presenting the same construction using the internal language provided by linear logic:
\begin{center}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$B \vdash B$}
\AxiomC{}
\UnaryInfC{$C \vdash C$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$B, B \multimap C \vdash C$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap B, B \multimap C \vdash C$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$A \multimap B, B \multimap C \vdash A \multimap C$}
\DisplayProof
\qquad
\tagarray{\label{comp_machine_intro}}
\end{center}
This syntactical object is called a \emph{proof}. Here $A,B,C$ are formal variables that we can think of as standing for unknown objects of $\cat{C}$ (or in fact any symmetric closed monoidal category) and $\multimap$ is a connective called linear implication. If we choose to specialise these variables to particular objects $a,b,c$ the proof acquires a ``shadow'' or interpretation in $\cat{C}$, namely, the internal composition \eqref{eq:comp_map_intro}. 
% In this syntactical object, called a \emph{proof} of the logic $\multimap$ is a connective, like disjunction $\lor$ and conjunction $\land$ in classical logic. Expressions like $B, B \multimap C \vdash C$ are called \emph{sequents}; this particular sequent stands for the proposition that $C$ may be deduced from hypotheses $B$ and $B \multimap C$. The proof is meant to be read from top to bottom, and the movement between rows is controlled by \emph{deduction rules} of the logic, labelled here as $\multimap L$ and $\multimap R$.

Even without formally defining what a proof is (this will be done in Section \ref{section:intro_ll}) one can start to see how this proof formalises the construction in \eqref{eq:construct_comp_catt}. For example the deduction rule $\multimap L$ corresponds to precomposition with an evaluation map, and $\multimap R$ to the use of adjunction. This demonstrates how linear logic formalises the construction of canonical maps in a symmetric closed monoidal category. Moreover, logic also dictates the \emph{relations} between canonical maps constructed in this way, which is the reason why logic often arises in connection with coherence questions \cite{??}.
\\

% To check a property of this internal composition, for example that it is associative, one would write down a commutative diagram whose vertices consists of terms like the last line of \eqref{eq:construct_comp_catt} and steadily decompose this into smaller commutative squares using properties of the adjunction maps, until one reaches trivialities involving identities $1_c \in \Hom_{\cat{C}}(c,c)$.

% But checking the commutativity of diagrams is not the aspect of logic that we wish to sell. This is not the complete language of linear logic. The multiplicative fragment of intuitionistic linear logic contains connectives $\multimap, \otimes$ and ``talks about'' canonical morphisms that can be constructed in a symmetric closed monoidal category. But from a logical point of view this system is not very interesting.

%To describe the more interesting parts, it will be helpful to talk about a proof as being a kind of \emph{machine}: for example, once we hand the proof \eqref{comp_machine_intro} a specification for objects $a,b,c$ of some category $\cat{C}$, we can imagine it methodically following the instructions embodied in the three downwards arrows of \eqref{eq:construct_comp_catt} until it has constructed the internal composition map relating $a,b,c$. The upshot is that our proof is a machine which composes arrows.

Happily, symmetric closed monoidal categories are the \emph{least} interesting part of linear logic. Things become more interesting when we start to talk about natural operations on morphisms in $\cat{C}$ that cannot be encoded by a diagram like \eqref{eq:construct_comp_catt} built up inductively from adjunction and pre- and post-composition with maps already constructed. For example, the operation which takes as input an object $a$, an endomorphism $f: a \lto a$, and returns the square $f \circ f$ has no such description (why?). However, this operation \emph{is} described by a proof in linear logic, namely:
\begin{center}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$A \multimap A, A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize der}
\UnaryInfC{$!( A \multimap A ), A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize der}
\UnaryInfC{$!( A \multimap A), !( A \multimap A) \vdash A \multimap A$}
\RightLabel{\scriptsize ctr}
\UnaryInfC{$!( A \multimap A) \vdash A \multimap A$}
\DisplayProof
\qquad
\tagarray{\label{church_2_intro}}
\end{center}
This proof involves a new connective $!$ called the \emph{exponential}, but we recognise that until the fourth line, this is the earlier proof with $A$ substituted for $B,C$. Since that proof was presenting the operation of composition, it only takes a small leap of imagination to see the proof \eqref{church_2_intro} as a gadget which takes a morphism $f: a \lto a$ and duplicates it, feeding $f,f$ into the composition operation to obtain $f \circ f$. The aim of this note is that, by the end, the reader will be able to look at \eqref{church_2_intro} and see all of this immediately.

The range of operations that can be defined using $!$ is significantly larger. For example any integer can be encoded as proof (in fact \eqref{church_2_intro} is an encoding of the number $2$) and addition and multiplication of integers can be also encoded. However for these operations to have an internal meaning in the same way that composition does, the category $\cat{C}$ needs to be equipped with additional structure. This turns out to be a comonad ${!}: \cat{C} \lto \cat{C}$ which interprets the exponential in the same way that the internal Hom $\multimap: \cat{C} \times \cat{C} \lto \cat{C}$ interprets linear implication. The category of (possibly infinite dimensional) vector spaces and linear maps is one such category, when it is equipped with the endofunctor $!$ sending a vector space to the cofree cocommutative coalgebra generated by it.

%But this is far from the most complicated machine that can be built with the language of linear logic. The reader will not be surprised to learn that for each integer $n$ there is a proof which is represented in the semantics by taking an endomorphism and raising it to the $n$th power. Let us denote this proof by $\underline{n}$. Further one can encode a class of \emph{functions} $f: \mathbb{N} \lto \mathbb{N}$ as proofs $\underline{f}$ in such a way that $\underline{f}$ acts on $\underline{n}$ to yield a proof of $\underline{f(n)}$. In this way we see that the \emph{set of proofs} has a rich and interesting structure. From one point of view it is these structured sets that are the subject of proof theory and related areas of computer science.\footnote{As evidence that this structure is complicated, quote implicit computational complexity.} %This can be seen as an example of the Curry-Howard correspondence, which matches proofs in certain logics with programs in programming languages like the $\lambda$-calculus.This perspective, which emphasises the structure of the \emph{set} of possible proofs, is emblematic of the age of computation: 



% This shift in perspective within logic, from truth to structure, is correlated with the increasing importance of computation, both as an economic activity and as a body of ideas. The deep connection between logic and computation is described by a famous principle, the Curry-Howard correspondence, which equates \emph{propositions} in intuionistic logic with \emph{types} in the sense of computer programming, and gives a bijection between the set of proofs of a given proposition and programs of the corresponding type. In particular, the set of computable functions $\mathbb{N} \lto \mathbb{N}$ is in bijection with the set of proofs of a certain proposition, and the division of the former into complexity classes (e.g. polynomial time or logarithmic time complexity) demonstrates clearly the mysteriousness and importance of the structure of sets of proofs as mathematical objects.

%TODO: mention complexity

% Return to symmetry. Concretely, this means that we will explain how to assign a vector space to every proposition in linear logic and a linear map to every proof, in a way which respects the structure and ``symmetries'' of the set of proofs. there is a diagrammatic formulation of proofs in linear logic (a more recent extension of the logic used by Gentzen) in which actions on the plane by isotopies generates, in addition to some more exotic relations, the relation of ``explication''. 

%For example, in \emph{linear logic}  any proof of a formula can be viewed as a decorated graph called a proof-net which is similar in some ways to a bordism or a string diagram, and the semantics of linear logic \cite{mellies} can be intuitively understood in terms of functors from a symmetric monoidal category of formulas (the objects) and proofs (the morphisms) to other monoidal categories, including the category of vector spaces.\footnote{Unfortunately there does not yet appear to be a formal statement in the literature to the effect that the set of categorical semantics of linear logic is in bijection with functors out of some free category generated by the syntax, but nonetheless we feel this is a helpful motivating perspective.}

%This invites a comparison to the subject of topological field theory \cite{atiyah,witten} which may be described as the study of monoidal functors from categories of bordisms to the category of vector spaces. However, there is a crucial difference which makes the semantics of linear logic more interesting in some ways than representations of a category of bordisms: proofs are \emph{nonlinear} objects, and their semantics is therefore highly infinite. To illustrate how this nonlinearity manifests itself in algebraic semantics, suppose we are given rings $A,B$ and a $B$-$A$-bimodule $X$. A basic operation is the linear functor
%\begin{equation}
%M \longmapsto X \otimes_A M
%\end{equation}
%from the category of left $A$-modules to the category of left $B$-modules. By analogy with the language of analysis, $X$ is sometimes referred to as a ``kernel''. This kind of operation on modules is fundamental in the algebra of higher-dimensional topological field theory where defects between field theories are represented by bimodules of various kinds \cite{??}.

%An operation that does not arise as naturally in that context is the following ``quadratic'' map defined on $A$-$A$-bimodules $X$ (that is, in the situation where $A = B$)
%\begin{equation}\label{eq:church_as_bimod}
%X \longmapsto X \otimes_A X\,.
%\end{equation}
%This operation is nonlinear and cannot be represented by tensoring with some bimodule. Nonetheless there \emph{is} a good notion of such nonlinear kernels: they are called \emph{programs}, and the operation above is an incarnation of the Church numeral $\church{2}$ in the $\lambda$-calculus. In this note we will discuss Church numerals and their semantics in the simple setting of vector spaces. Making the above comments about bimodules and programs precise requires the construction of \emph{bicategorical} semantics of linear logic, where the ``squaring program'' $\church{2}$ of \eqref{eq:church_as_bimod} can be interpreted as a kind of bundle over the space parametrising suitable bimodules $X$, whose fibre over a point $[X]$ is the square $X \otimes_A X$. This idea will be made precise in \cite{??,??} using matrix factorisations (which are bimodules equipped with an additional operator) and a bicategory which extends the bicategory of Landau-Ginzburg models studied in the context of topological field theory \cite{??}. 

% We take the point of view that semantics of linear logic ``lurks'' just beyond the boundaries of subjects of topological field theory, and interaction between the two subjects is likely to be rewarding for both.

% These parametrising spaces are the ``infinite'' content reflecting the non-linearity, but they are finite in the context of algebraic geometry where they are cut out of affine spaces $\mathbb{C}^n$ by finite numbers of polynomial equations.

% Such novel semantics defined using algebra might be of interest of a proof theorist, but what is there in logic for the algebraist? The author cannot presume to speak for experts in logic, but he can explain the source of his own fascination with the subject. After the work of Turing, G\"odel and Church \cite{soare} computation has become a fundamental concept in mathematics, and we have a good understanding of which functions are computable. But what kind of thing is the \emph{process} of computation? One answer is that computation is what happens when a Turing machine is iterated, during the $\beta$-reduction process of the $\lambda$-calculus, or during the cut-elimination process of logic. These models of computation are equivalent, but each is tied to specific syntax. What is the common essence of these processes? This is a question as deep as ``What is space?'' and it would be absurd to expect conclusive answers \cite{denning}. Nonetheless such questions generate interesting mathematics, a recent example being homotopy type theory \cite{univalent,warren}, and the tools developed in the last few decades of research into higher categories, topological field theory and algebraic geometry offer many interesting avenues of concrete investigation.

%The first aspect of logic that we choose for special emphasis is the already mentioned \emph{nonlinearity} represented in linear logic by the exponential modality, which corresponds in the vector space semantics to forming from a vector space $V$ the universal cocommutative coalgebra ${!}V$ whose coproduct is the universal way of duplicating elements of $V$. Through an embedding of a prototypical programming language called the $\lambda$-calculus into linear logic we will see how the exponential modality is the aspect of logic responsible for the capability of programs to use their inputs more than once.

% This reuse -- which is demonstrated in the ``program'' \eqref{eq:church_as_bimod} which uses the input bimodule $M$ twice -- is a form of non-linearity, which is moreover the source of all the interesting complexity of non-trivial programs.\footnote{It might seem odd that the first aspect of linear logic that we choose to emphasise is non-linearity. But the point of linear logic is not that everything is linear. Classical logic is a mix of linear and non-linear components, and the insight of Girard with linear logic is that it is possible to create a refinement where there is an explicit boundary between these two worlds, and there are special language features -- such as the exponential modality -- to manage transitions across this boundary.}

%The second particularly interesting aspect of logic is \emph{cut-elimination}. This is a fundamental feature of logic going back to Gentzen \cite{gentzen} which may be described (superficially) as the answer to the following question: given two composable functions $f,g$ given by algorithms, what is the simplest form of the algorithm computing $g \circ f$? LINK BACK to earlier comments about what is computation This is linked to deep questions about the nature of computation, as elucidated by Girard \cite{girard_towards}.

%Here is how we imagine potential readers of this article: either a computer scientist or logician interested in the semantics of the exponential modality in linear logic and connections between proof theory and geometry, or an algebraist or mathematical physicist who likes symmetric monoidal categories and is curious about the boundary between the finite (in the sense of dualisability of objects in topological field theory) and the infinite (in the sense of infinite coproducts) and who might appreciate learning that logicians have their own way of probing the realm between finite and infinite, with deep roots. 

TODO
Cite Section 2.4.2 of Benton for abelian groups

\section{Computer programs and the $\lambda$-calculus}\label{section:lambda_calc}

Around the same time as Turing defined his machines, Church gave a different formalisation of the idea of a computable function called the $\lambda$-calculus \cite{church,selinger}. Both concepts identify the same class of computable functions $\mathbb{N} \lto \mathbb{N}$, but the $\lambda$-calculus is more natural from the point of category theory, and it serves as the theoretical underpinning of functional programming languages like Lisp \cite{mccarthy} and Haskell. Intuitively, while Turing machines make precise the concept of \emph{logical state} and \emph{state transition}, the $\lambda$-calculus captures the concepts of \emph{variables} and \emph{substitution}. 

The $\lambda$-calculus is determined by its terms and a rewrite rule on those terms. The terms are to be thought of as programs, and the rewrite rule as the method of execution of programs. A \emph{term} in the $\lambda$-calculus is either one of a countable number of variables $x,y,z,\ldots$ or an expression of the type
\begin{equation}
(M \; N) \quad \text{or} \quad (\lambda x\,.\, M)
\end{equation}
where $M,N$ are terms and $x$ is any variable. The terms of the first type are called \emph{function applications} while those of the second type are called \emph{lambda abstractions}. An example of a term, or program, that will be important throughout this note is
\begin{equation}
T := ( \lambda y \,.\, ( \lambda x \,.\, (y \,(y \; x))))\,.
\end{equation}
Note that the particular variables chosen are not important, but the pattern of occurrences of the \emph{same} variable certainly is. That is to say, we deal throughout with terms up to an equivalence relation called $\alpha$-conversion, under which for example $T$ is equivalent to the term $( \lambda z \,.\, ( \lambda t \,.\, (z \,(z \; t))))$. Following standard practice, we will adopt this convention.

If we are supposed to think of $T$ as a program, we must describe what this program \emph{does}. The dynamic content of the $\lambda$-calculus arises from a rewrite rule called \emph{$\beta$-reduction} generated by the following basic rewrite rule
\begin{equation}\label{eq:basic_beta_reduction}
( (\lambda x \,.\, M)\, N) \longrightarrow_\beta M[N/x]
\end{equation}
where $M,N$ are terms, $x$ is a variable, and $M[N/x]$ denotes the term $M$ with all free occurrences of $x$ replaced by $N$.\footnote{There is a slight subtlety here since we may have to rename variables in order to avoid free variables in $N$ being ``captured'' as a result of this substitution, see \cite[\S 2.3]{selinger}.} We write $A \rightarrow_\beta B$ if the term $B$ is obtained from $A$ by rewriting a sub-term of $A$ according to the rule \eqref{eq:basic_beta_reduction}. The smallest reflexive, transitive and symmetric relation containing $\rightarrow_\beta$ is called \emph{$\beta$-equivalence}, written $M =_{\beta} N$ \cite[\S 2.5]{selinger}.

We think of the lambda abstraction $(\lambda x \,.\, M)$ as a program with input $x$ and body $M$, so that the $\beta$-reduction step in \eqref{eq:basic_beta_reduction} has the interpretation of our program being fed the input term $N$ which is subsequently bound to $x$ throughout $M$. A term is \emph{normal} if there are no sub-terms of the type on the left hand side of \eqref{eq:basic_beta_reduction}. In the $\lambda$-calculus computation occurs when two terms are coupled by a function application in such a way as to create a term which is not in normal form: then $\beta$-reductions are performed until a normal form (the output of the computation) is reached.\footnote{Not every $\lambda$-term may be reduced to a normal form by $\beta$-reduction because this process does not necessarily terminate. However if it does terminate then the resulting reduced term is canonically associated to the original term; this is the content of Church-Rosser theorem \cite[\S 4.2]{selinger}.}

In terms of the rewriting of terms generated by $\beta$-reduction, let us now examine what the program $T$ does when fed another term. For a term $M$, we have
\begin{equation}\label{eq:beta_reduc_dup}
(T \, M) = (( \lambda y \,.\, ( \lambda x \,.\, (y \,(y \; x))))\, M) \longrightarrow_\beta (\lambda x \, . \, (M \, (M \; x)))\,.
\end{equation}
Thus $(T \, M)$ behaves like the square of $M$, in the sense that it is a program which takes a single input $x$ and returns $(M \, (M \; x))$. If we modify $T$ by nesting more than two function applications, we can define for each integer $n \ge 0$ a term $\underline{n}$ called the $n$th \emph{Church numeral} \cite[\S 3.2]{selinger}. This program has the property that $(\underline{n} \, M)$ behaves like the $n$th power of $M$. The Church numberal $\church{2}$, which is our old friend $T$, will be our object of study throughout this note.\footnote{To put this into a broader context: once the integers have been translated into terms in the $\lambda$-calculus it makes sense to say that a program $F$ \emph{computes} a function $f: \mathbb{N} \lto \mathbb{N}$ if for each $n \ge 0$ we have $(F \; \underline{n}) =_\beta \underline{f(n)}$. A function $f$ is called \emph{computable} if there is such a term $F$.}
%\footnote{This class of functions agrees with the class of functions $\mathbb{N} \lto \mathbb{N}$ which may be implemented on a Turing machine, and with G\"odel's general recursive functions. That these classes all match the intuitive notion of what it means for a function to be computable is the belief known as the Church-Turing thesis.}
%Let's see what happens when we feed this resulting program some other input $N$
%\begin{align*}
%((T \, M) \, N) &\longrightarrow_\beta ((\lambda x \, . \, (M \, (M \; x))) \, N) %\longrightarrow_\beta (M \, (M \; N))\,.
%\end{align*}

We have now defined the $\lambda$-calculus and introduced our basic example of a program, the Church numeral $\church{2}$. The emphasis in this article is on the structure of sets of proofs, and this is turns out to be closely related to the structure of sets of \emph{programs}, that is, $\lambda$-terms. Let $D$ denote the set of all $\beta$-equivalence classes of $\lambda$-terms. This is a fabulously complicated set: consider that for example $T$ defines a function
\begin{gather*}
T: D \lto D,\\
[M] \mapsto [(T \, M)]\,.
\end{gather*}
and the same is true of any term. The discovery of the first mathematical model of this structure by Dana Scott \cite{scott} was an important milestone in the development of computer science, and it marked the beginning of the field called \emph{denotational semantics}. For our purposes, however, the full $\lambda$-calculus is \emph{too} complicated. There is a much weaker version called the simply-typed $\lambda$-calculus which is more closely related to linear logic.

The reader familiar with programming will know that functions in many programming languages are \emph{typed} by their input and output specification. For instance, a function computing the $n$th Fibonacci number in the language C takes an input of type ``int'' -- the integer $n$ -- and returns an output also of type ``int''. In the \emph{simply-typed} $\lambda$-calculus the Church numeral $\church{2}$ is the same as before, but with type annotations
\[
T_{typed} := ( \lambda y^{A \rightarrow A} \,.\, ( \lambda x^A \,.\, (y \,(y \; x))))\,.
\]
These annotations indicate that the first input $y$ is restricted to be of the type of a function from $A$ to $A$ (that is, of type $A \rightarrow A$) while $x$ is restricted to be of type $A$. Overall $T_{typed}$ takes a term of type $A \rightarrow A$ and returns another term of the same type, so it is a term of type $(A \rightarrow A) \rightarrow (A \rightarrow A)$. For a more complete discussion see \cite[\S 6]{selinger}. 

%Now if we let $[A \rightarrow A]$ denote the set of all $\beta$-equivalence classes of terms of type $A \rightarrow A$ then $T_{typed}$ defines a function
%\begin{gather*}
%T_{typed}: [A \rightarrow A] \lto [A \rightarrow A]\,,\\
%[M] \mapsto [(T_{typed}\, M)]\,.
%\end{gather*}

%This motivates the following question, which we will keep in mind during the subsequent discussion of linear logic:

%\begin{question}\label{question:lambda} Is there a representation of programs in the simply-typed $\lambda$-calculus, for example $T_{typed}$, as linear maps between vector spaces?
%\end{question}

%We have in mind something more interesting than linear maps between free vector spaces on inscrutable sets like $D$. The obstruction to realising this aim is clear: it would be natural to associate to $T_{typed}$ the map $\alpha \mapsto \alpha^2$ on endomorphisms of some vector space $V$, but this is not linear. 

% We begin with a brief introduction to the $\lambda$-calculus (Section \ref{section:lambda_calc}) and introduce our basic example, the Church numeral $\church{2}$. Then we move on to discuss intuitionistic logic (Section \ref{section:intuit_logic}) and the Curry-Howard correspondence which shows that the simply-typed $\lambda$-calculus is isomorphic to intuitionistic logic, under the slogan ``proofs are programs''. As foreshadowed in the introduction, we pay particular attention to the role of nonlinearity of proofs and programs, which manifests itself in the role of duplication of terms in the $\lambda$-calculus and the contraction rule in logic. This nonlinearity, implicit in classical logic, is made explicit in a refinement of classical logic due to Girard, called Linear Logic (Section \ref{section:intro_ll}). For more details see \cite{girard_prooftypes,mellies} or, for the reader with a lunch break to spare \cite{scott_talk}.

% At the same time as Turing defined his machines, Church gave another formalisation of the intuitive idea of a computable function in terms of what he called the $\lambda$-calculus \cite{church,selinger}. Both concepts identify the same class of computable functions $\mathbb{N} \lto \mathbb{N}$, but the $\lambda$-calculus is much more natural from the point of category theory, and it also serves as the theoretical underpinning of programming languages like Lisp \cite{mccarthy}. Intuitively, while Turing machines make precise the concept of \emph{logical state} and \emph{state transition}, the $\lambda$-calculus captures in a precise mathematical form the concepts of \emph{variables} and \emph{substitution}. 

\section{Linear logic}\label{section:intro_ll}

\setlength{\epigraphwidth}{0.8\textwidth}
\epigraph{Linear Logic is based on the idea of resources, an idea violently negated by the contraction rule. The contraction rule states precisely that a resource is potentially infinite, which is often a sensible hypothesis, but not always. The symbol $!$ can be used precisely to distinguish those resources for which there are no limitations. From a computational point of view $!A$ means that the datum $A$ is stored in the memory and may be referenced an unlimited number of times. In some sense, $!A$ means forever.}
{Jean-Yves Girard, Andre Scedrov, Philip J. Scott, \textit{Bounded linear logic}}

% The standard way to introduce linear logic is to compare it to classical logic, but to take this route we would have to assume familiarity with the sequent calculus for intuitionistic logic, which we do not wish to do. Instead, reversing the usual order of presentation, we will explain linear logic as the language of a closed symmetric monoidal category $\cat{C}$ equipped with a 

There are two main styles of formal mathematical proofs: Hilbert style systems, which most mathematicians will be exposed to as undergraduates, and natural deduction or sequent calculus systems, which are taught to students of computer science. What these two styles share is that they are about propositions (or sequents) and their proofs, which are constructed from axioms via deduction rules. The differences lie in the way that proofs are formatted and manipulated as objects on the page. In this note we will consider only the sequent calculus style of logic, since it is more naturally connected to category theory. The proofs \eqref{comp_machine_intro} and \eqref{church_2_intro} from the introduction are examples of such proofs.

In \emph{linear logic}\footnote{We consider propositional intuitionistic linear logic without additives, with the exception of Section \ref{??} where we add quantifiers.} there are propositional variables $x,y,z,\ldots$, two binary connectives $\multimap$ (linear implication), $\otimes$ (tensor) and a single unary connective $!$ (the exponential). There is a single constant $1$. The set of \emph{formulas} is defined recursively as follows: any propositional variable or constant is a formula, and if $A,B$ are formulas then so are
\[
A \multimap B, \qquad A \otimes B, \qquad {!}A\,.
\]
An important example of a formula is $\inta_A$ defined for any formula $A$ as
\begin{equation}\label{defn:integers} 
\inta_A = {!}( A \multimap A) \multimap (A \multimap A)\,.
\end{equation}
For reasons that will become clear, $\inta_A$ is referred to the type of \emph{integers on $A$} (throughout \emph{type} is used as a synonym for formula). A \emph{sequent} is an expression of the form
\[
A_1,\ldots,A_n \vdash B
\]
with formulas $A_1,\ldots,A_n, B$ of the logic connected by a \emph{turnstile} $\vdash$. The intuitive reading of this sequent is the proposition that $B$ may be deduced from the hypotheses $A_1,\ldots,A_n$. The letters $\Gamma, \Delta$ are used to stand for arbitrary sequences of formulas, possibly empty. A \emph{proof} of a sequent is a series of deductions, beginning from tautologous \emph{axioms} of the form $A \vdash A$, which terminates with the given sequent. At each step of the proof the deduction must follow a list of \emph{deduction rules}. 

More precisely, let us define a \emph{pre-proof} to be a rooted tree whose edges are labelled with sequents. In order to follow the logical ordering, the tree is presented with its root vertex at the bottom of the page, and we orient edges towards the root (so downwards). The labels on incoming edges at a vertex are called \emph{hypotheses} and on the outgoing edge the \emph{conclusion}. For example consider the following tree, and its equivalent presentation in sequent calculus notation:
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\begin{tikzpicture}[scale=0.6,auto]
\node (topl) at (-2,2) {$\Gamma \vdash A$};
\node (topr) at (2,2) {$\Delta \vdash B$};
\node (bottomm) at (0,-2) {$\Gamma, \Delta \vdash A \otimes B$};
\node (mid) at (0,0) {};
\draw (bottomm) -- (mid);
\draw (mid) -- (topr);
\draw (mid) -- (topl);
\end{tikzpicture}
&
\AxiomC{$\Gamma \vdash A$} \AxiomC{$\Delta \vdash B$}
\BinaryInfC{$\Gamma, \Delta \vdash A \otimes B$}
\DisplayProof
\tagarray{\label{example_tree}}
\end{tabular}
\end{center}
Leaves are presented in the sequent calculus notation with an empty numerator.

\begin{definition} A \emph{proof} is a pre-proof together with a compatible labelling of vertices by deduction rules. The list of deduction rules is given in the first column of \eqref{deduction_rule_ax} -- \eqref{deduction_rule_right1}. A labelling is \emph{compatible} if at each vertex, the sequents labelling the incident edges match the format displayed in the deduction rule.
\end{definition}

In all deduction rules, the sets $\Gamma$ and $\Delta$ may be empty and, in particular, the promotion rule may be used with an empty premise. In the promotion rule, ${!} \Gamma$ stands for a list of formulas each of which is preceeded by an exponential modality, for example ${!}A_1,\ldots, {!}A_n$. The diagrams on the right are string diagrams and should be ignored until Section \ref{section:diagrammatics}. In particular they are \emph{not} the trees associated to proofs.

% The linear logic of Girard \cite{girard_llogic, mellies} is a refinement of classical logic in which the contraction rule is restricted to formulas that are prefixed with a new unary connective ${!}$ called the exponential modality. A hypothesis ${!} A$ may be used in a proof infinitely many times as in classical logic, but a bare formula $A$ may only be used once\footnote{To an algebraist, this may sound as strange as an admonition to only use Nakayama's lemma once a day. After all, isn't the truth endlessly reusable? In reply, we would ask the reader to consider again the interpretation of the proof $\gamma$ in \eqref{integrate_against_a_proof} as a function mapping input proofs to output proofs. The act of computing these outputs is a physical process, whether it takes place in a machine or a human mind, and it involves the allocation and deallocation of finite resources. The insight of Girard with linear logic is that far from being implementation or engineering details, these acts of allocation and deallocation are in fact logical, and of fundamental importance.} (the proof is \emph{linear} in $A$). The other connectives are also refined, for instance $\Rightarrow$ is refined to a linear implication $\multimap$, with the former being recovered from the latter by the translation $A \Rightarrow B = {!} A \multimap B$.

% We consider propositional intuitionistic linear logic without additives (henceforth referred to simply as \emph{linear logic}). The adjective \emph{intuitionistic} means that the right hand side of a sequent is constrained to have precisely one formula, while \emph{propositional} means that we omit quantifiers.\footnote{The additives may be included in the obvious way, we omit them simply because our examples do not involve them. At the moment we do not understand quantifiers.} 

% Axiom rule
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{}
\LeftLabel{(Axiom): }
\UnaryInfC{$A \vdash A$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (top) at (0,1) {$A$};
\node (bottom) at (0,-1) {$A$};
\draw (bottom) -- (top);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_ax}}
\end{tabular}
\end{center}

% Exchange rule ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$\Gamma, A, B, \Delta \vdash C$}
\LeftLabel{(Exchange): }
\UnaryInfC{$\Gamma, B, A, \Delta \vdash C$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (gamma) at (-2,-4) {$\Gamma$};
\node (a) at (1,-4) {$A$};
\node (b) at (-1,-4) {$B$};
\node (delta) at (2,-4) {$\Delta$};
\node (top) at (0,2) {$C$};
\draw (0,0) -- (top);
\bluenode (o) at (0,0) {};
\draw[out=90,in=180] (-2,-2) to (o);
\draw (gamma) -- (-2,-2);
\draw[out=90,in=225] (-1,-1.5) to (o);
\draw[out=90,in=270] (a) to (-1,-1.5);
\draw[out=90,in=315] (1,-1.5) to (o);
\draw[out=90,in=270] (b) to (1,-1.5);
\draw[out=90,in=0] (delta) to (o);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_ex}}
\end{tabular}
\end{center}

% Cut rule ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$\Gamma \vdash A$} 
\AxiomC{$A, \Delta \vdash B$}
\LeftLabel{(Cut): }
\RightLabel{\scriptsize cut}
\BinaryInfC{$\Gamma, \Delta \vdash B$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (top) at (0,4) {$B$};
\node (bottoml) at (-2,-2) {$\Gamma$};
\node (bottomr) at (2,-2) {$\Delta$};
\bluenode (1) at (-2,0) {};
\bluenode (2) at (0,2) {};
\draw (bottoml) -- (1);
\draw[out=90,in=180] (1) to node {$A$} (2);
\draw[out=90,in=0] (bottomr) to (2);
\draw (2) -- (top);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_cut}}
\end{tabular}
\end{center}


% Right tensor ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$\Gamma \vdash A$} \AxiomC{$\Delta \vdash B$}
\LeftLabel{(Right $\otimes$): }
\RightLabel{\scriptsize $\otimes$-$R$}
\BinaryInfC{$\Gamma, \Delta \vdash A \otimes B$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (topl) at (-1,2) {$A$};
\node (topr) at (1,2) {$B$};
\node (bottoml) at (-1,-2) {$\Gamma$};
\node (bottomr) at (1,-2) {$\Delta$};
\bluenode (l) at (-1,0) {};
\bluenode (r) at (1,0) {};
\draw (bottoml) -- (l);
\draw (l) -- (topl);
\draw (bottomr) -- (r);
\draw (r) -- (topr);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_righttensor}}
\end{tabular}
\end{center}

% Left tensor ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$\Gamma, A, B \vdash C$}
\LeftLabel{(Left $\otimes$): }
\RightLabel{\scriptsize $\otimes$-$L$}
\UnaryInfC{$\Gamma, A \otimes B \vdash C$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (gamma) at (-2,-4) {$\Gamma$};
\node (mid) at (0,-4) {$A \otimes B$};
\node (delta) at (2,-4) {$\Delta$};
\node (top) at (0,2) {$C$};
\draw (0,0) -- (top);
\bluenode (o) at (0,0) {};
\draw[out=90,in=180] (-2,-2) to (o);
\draw (gamma) -- (-2,-2);
\draw (-1,-1.5) to (1,-1.5);
\draw[out=90,in=225] (-1,-1.5) to (o);
\draw (mid) to (0,-1.5);
\draw[out=90,in=315] (1,-1.5) to (o);
\draw[out=90,in=0] (delta) to (o);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_lefttensor}}
\end{tabular}
\end{center}

% Right Hom ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$A, \Gamma \vdash B$}
\LeftLabel{(Right $\multimap$): }
\RightLabel{\scriptsize $\multimap R$ }
\UnaryInfC{$\Gamma \vdash A \multimap B$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (topr) at (1,4) {$A \multimap B$};
\bluenode (o) at (1,0) {};
\node (gamma) at (2.5,-4) {$\Gamma$};
\draw[out=90,in=0] (gamma) to (o);
\draw[out=0,in=180] (-1.5,-2) to node {$A$} (o);
\draw[out=180,in=270] (-1.5,-2) to (-3,0);
\draw[out=90,in=180] (-3,0) to (1,2);
\draw (o) to node [swap] {$B$} (1,2);
\draw (1,2) to (topr);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_righthom}}
\end{tabular}
\end{center}

% Left Hom ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$\Gamma \vdash A$} \AxiomC{$B, \Delta \vdash C$}
\LeftLabel{(Left $\multimap$): }
\RightLabel{\scriptsize $\multimap L$ }
\BinaryInfC{$\Gamma, A \multimap B, \Delta \vdash C$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (topr) at (1,2) {$C$};
\bluenode (q) at (-3,-4) {};
\bluenode (o) at (1,0) {};
\node (delta) at (3.5,-6) {$\Delta$};
\node (gamma) at (-3, -6) {$\Gamma$};
\node (ab) at (1,-6) {$A \multimap B$};
\draw[out=90,in=0] (delta) to (o);
\draw (o) to (topr);
\draw[out=90,in=180] (-1,-2) to node {$B$} (o);
\draw[out=90,in=270] (gamma) to (q);
\draw[out=90,in=180] (q) to node {$A$} (-1,-2);
\draw[out=90,in=0] (ab) to (-1,-2);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_lefthom}}
\end{tabular}
\end{center}

% Promotion ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$! \Gamma \vdash A$}
\LeftLabel{(Promotion): }
\RightLabel{\scriptsize prom}
\UnaryInfC{$!\Gamma \vdash !A$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (top) at (0,4) {$!A$};
\node (bottom) at (0,-4) {$!\Gamma$};
\bluenode (o) at (0,0) {};
\draw (o) to node {$A$} (0,2);
\drawbang (bottom) -- (0,-2);
\drawbang (0,2) -- (top);
\drawbang (0,-2) -- (o);
\drawprom (0,0) ellipse (2cm and 2cm);
\dernode (bottom) at (0,-2) {};
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_prom}}
\end{tabular}
\end{center}

% Dereliction ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$\Gamma, A \vdash B$}
\LeftLabel{(Dereliction): }
\RightLabel{\scriptsize der}
\UnaryInfC{$\Gamma, !A \vdash B$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (top) at (0,2) {$B$};
\bluenode (o) at (0,0) {};
\dernode (d) at (2,-2) {};
\node (gamma) at (-2, -4) {$\Gamma$};
\node (banga) at (2,-4) {$!A$};
\drawbang (banga) -- (d);
\draw (o) -- (top);
\draw[out=90,in=0] (d) to node [swap] {$A$} (o);
\draw (gamma) to (-2,-2);
\draw[out=90,in=180] (-2,-2) to (o);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_der}}
\end{tabular}
\end{center}

% Contraction ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$\Gamma, !A, !A \vdash B$}
\LeftLabel{(Contraction): }
\RightLabel{\scriptsize ctr}
\UnaryInfC{$\Gamma, !A \vdash B$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (top) at (0,2) {$B$};
\bluenode (o) at (0,0) {};
\node (gamma) at (-2, -4) {$\Gamma$};
\node (banga) at (2,-4) {$!A$};
\drawbang (banga) -- (2,-2);
\draw (o) -- (top);
\drawbang[out=90,in=0] (2,-2) to (o);
\drawbang[out=180,in=270] (2,-2) to (o);
\draw (gamma) to (-2,-2);
\draw[out=90,in=180] (-2,-2) to (o);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_contr}}
\end{tabular}
\end{center}

% Weakening ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$\Gamma \vdash B$}
\LeftLabel{(Weakening): }
\RightLabel{\scriptsize weak}
\UnaryInfC{$\Gamma, !A \vdash B$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (topl) at (-1,2) {$B$};
\node (bottoml) at (-1,-2) {$\Gamma$};
\node (bottomr) at (1,-2) {$!A$};
\bluenode (l) at (-1,0) {};
\draw (bottoml) -- (l);
\draw (l) -- (topl);
\drawbang (bottomr) -- (1,0);
\node[circle,draw=teal!50,fill=teal!50,inner sep=0.5mm] at (1,0) {};
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_weak}}
\end{tabular}
\end{center}

% Left 1 ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{$\Gamma \vdash A$}
\LeftLabel{(Left $1$): }
\RightLabel{\scriptsize $1$-$L$}
\UnaryInfC{$\Gamma, 1 \vdash A$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (topl) at (-1,3) {$B$};
\node (bottoml) at (-1,-3) {$\Gamma$};
\node (bottomr) at (1,-3) {$k$};
\bluenode (l) at (-1,1) {};
\draw (bottoml) -- (l);
\draw (l) -- (topl);
\draw[out=90,in=0] (bottomr) to (-1,-0.5);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_left1}}
\end{tabular}
\end{center}

% Right 1 ======
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{2cm}}
\AxiomC{}
\LeftLabel{(Right $1$): }
\RightLabel{\scriptsize $1$-$R$}
\UnaryInfC{$\vdash 1$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.6,auto]
\node (top) at (0,1) {$k$};
\node (bottom) at (0,-1) {$k$};
\draw (bottom) -- (top);
\end{tikzpicture}
&
\tagarray{\label{deduction_rule_right1}}
\end{tabular}
\end{center}

% For the examples in this paper, the relevant deduction rules are the axiom rule, the cut rule, the right and left $\multimap$ introduction rules, the contraction rule and the promotion and dereliction rules. 

What about permutations? I mean the deduction rules need things in certain places.

\begin{example}\label{example:first_occur_2} For any formula $A$ let $\church{2}_A$ denote the proof \eqref{church_2_intro} from Section \ref{section:sketch}, which we repeat here for the reader's convenience:
\begin{center}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$A \multimap A, A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize der}
\UnaryInfC{$!( A \multimap A ), A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize der}
\UnaryInfC{$!( A \multimap A), !(A \multimap A) \vdash A \multimap $}
\RightLabel{\scriptsize ctr}
\UnaryInfC{$!( A \multimap A) \vdash A \multimap A$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$\vdash \inta_A$}
\DisplayProof
\qquad
\tagarray{\label{church_2_prooftree}}
\end{center}
For convenience in what follows we will also write $\church{2}_A$ for the proof of ${!}( A \multimap A ) \vdash A \multimap A$ obtained by reading the above proof up to the penultimate line. For each integer $n \ge 0$ there is a proof $\church{n}_A$ of $\inta_A$ constructed along similar lines, see \cite[\S 5.3.2]{girard_llogic} and \cite[\S 3.1]{danos}.
\end{example}
%  Although the proof obviously has a structure similar to \eqref{church_2_prooftree_classic} notice that there is a conversion between $A \multimap A$ and its infinite version ${!}(A \multimap A)$, where the duplication occurs. 

In what sense is this proof an avatar of the numeral $2$? The same question made sense in the context of the $\lambda$-calculus. There, we only appreciated the relationship between the term $T$ in \eqref{??} and the numeral $2$ when we saw how $T$ \emph{interacted} with other terms $M$ by forming the function application $(T \, M)$ and then finding a normal form with respect to $\beta$-equivalence. The analogue of function application in linear logic is the cut rule. The analogue of $\beta$-equivalence is an equivalence relation on the set of proofs of any sequent, which we write as $\rho =_{cut} \delta$ and communicate by saying that $\rho$ and $\delta$ are \emph{equivalent under cut-elimination}. The details of this relation will be explained later, but for now let us examine how it gives a ``dynamic'' meaning to the proof $\church{2}_A$. 

\begin{example}\label{example:cutagainst2} Let $\pi$ be any proof of $A \vdash A$ and consider the proof 
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{4cm}}
\AxiomC{$\pi$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\vdash A \multimap A$}
\RightLabel{\scriptsize prom}
\UnaryInfC{$\vdash {!}(A \multimap A)$}
\def\extraVskip{2pt}
\AxiomC{$\church{2}_A$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!(A \multimap A) \vdash A \multimap A$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\vdash A \multimap A$}
\DisplayProof
&

\tagarray{\label{cut_against_2_early}}
\end{tabular}
\end{center} 
We write $\delta \l \rho$ for the cut rule applied with left branch $\rho$ and right branch $\delta$, and $\operatorname{prom}( \pi )$ for the left hand branch of \eqref{cut_against_2_early}, so that the whole proof is denoted $\church{2}_A \l \operatorname{prom}( \pi )$.

Intuitively, the promotion rule ``makes perennial'' the data of the proof $\pi$ and prepares it to be used more than once (that is, in a \emph{nonlinear} way). The cut rule is logically the incarnation of Modus Ponens, and from the point of view of categorical semantics is the linguistic antecedent of composition. In the context of \eqref{cut_against_2_early} it plays the role of ``feeding'' the perennial version of $\pi$ as input to $\church{2}_A$. After our experience with the $\lambda$-term $T$ it is not a surprise that the above proof is equivalent under cut-elimination to
\begin{center}
\begin{tabular}{>{\centering}m{6cm} >{\centering}m{4cm}}
\AxiomC{$\pi$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$A \vdash A$}
\def\extraVskip{2pt}
\AxiomC{$\pi$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$A \vdash A$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$A \vdash A$}
\DisplayProof
&

\tagarray{\label{cut_against_2_early_3}}
\end{tabular}
\end{center} 
which we write as $\pi \l \pi$. 
\end{example}

With $\pi$ standing for the term $M$ in Section \ref{??}, we see the close analogy between the program $T$ of $\lambda$-calculus and the proof $\church{2}_A$ of linear logic, with the cut \eqref{cut_against_2_early} playing the role of $(T \, M)$ and cut-elimination the role of $\beta$-reduction. This is one aspect of the \emph{Curry-Howard correspondence} which relates programs in the simply-typed $\lambda$-calculus (and its extensions) to proofs in intuitionistic logic (and its extensions) with cut-elimination playing the role of $\beta$-reduction; see \cite{??}.

%As in intuitionistic logic, cut-elimination in linear logic generates an equivalence relation on proofs of a given sequent. This equivalence relation plays a role conceptually analogous to diffeomorphism of bordisms in topological field theory, but is more complicated because of the presence of the exponential modality, as for example the diagrammatic transformations in Section \ref{section:cut_elim} below demonstrate. (TODO - is this our first mention of cut-elimination)%We will have more to say about cut-elimination for linear logic in Section \ref{section:cut_elim} and Section \ref{section:goi}.

\section{Semantics of linear logic}\label{section:diagrammatics}

As we have already mentioned, on the set of proofs of any sequent $\Gamma \vdash A$ there is an equivalence relation called \emph{cut-elimination}, see Section \ref{??}.

One way to elaborate on the reason \emph{why} \eqref{cut_against_2_early} is equivalent to \eqref{cut_against_2_early_3} would be to list the rewrite rules that make up the equivalence relation. However we take an alternative approach: in the next section we develop the semantics of linear logic and explain how to reframe the discussion of $\church{2}_A$ in terms of an associated linear map of vector spaces.

Return to the structure of collections of proofs

 Writing $[ A \vdash A ]$ for the set of equivalence classes of proofs of $A \vdash A$ notice how from the proof $\church{2}_A$ we have constructed a \emph{function}, namely 
\begin{gather*}
[A \vdash A] \lto [A \vdash A]\,,\\
[\pi] \mapsto [ \church{2}_A \l \operatorname{prom}( \pi ) ] = [ \pi \l \pi ]\,.
\end{gather*}

A \emph{categorical semantics} of linear logic \cite{mellies, blue_book} assigns to each type $A$ an object $\den{A}$ of some category and to each proof of $\Gamma \vdash A$ a morphism $\den{\Gamma} \lto \den{A}$ in such a way that two proofs equivalent under cut-elimination are assigned the same morphism; these objects and morphisms are called \emph{denotations}. The connectives of linear logic become structure on the category of denotations, and compatibility with cut-elimination imposes identities relating these structures to one another. 

The upshot is that to define a categorical semantics the category of denotations must be a closed symmetric monoidal category equipped with a comonad, which is used to model the exponential modality \cite[\S 7]{mellies}. This is a refinement of the equivalence between simply-typed $\lambda$-calculus and cartesian closed categories due to Lambek and Scott \cite{lambek}. The first semantics of linear logic were the coherence spaces of Girard \cite[\S 3]{girard_llogic} which are a refined form of Scott's model of the $\lambda$-calculus. Models of full linear logic with negation involve the $\star$-autonomous categories of Barr \cite{barr_auto,barr_acc,barr_autolin} and the extension to include quantifiers involves indexed monoidal categories \cite{seely}.

\subsection{Denotations of formulas}

In this paper denotations all take place in the category $\cat{V}$ of $k$-vector spaces. Throughout $k$ is an algebraically closed field of characteristic zero. We explain the denotations of types now, and leave the denotation of proofs to the next section. To this end, for a vector space $V$ let ${!} V$ denote the cofree cocommutative coalgebra generated by $V$. We will discuss the explicit form of this coalgebra in the next section; for the moment it is enough to know that it exists and is determined up to unique isomorphism.

\begin{definition}\label{defn:denotation_objects} The \emph{denotation} $\den{A}$ of a type $A$ is defined inductively as follows:
\begin{itemize}
\item The propositional variables $x, y, z, \ldots$ are assigned chosen finite-dimensional vector spaces $\den{x}, \den{y}, \den{z}, \ldots$;
\item $\den{1} = k$;
\item $\den{A \otimes B} = \den{A} \otimes \den{B}$;
\item $\den{A \multimap B} = \den{A} \multimap \den{B}$ which is notation for $\Hom_k(\den{A}, \den{B})$;
\item $\den{!A} = {!} \den{A}$.
\end{itemize}
The denotation of a group of formulas $\Gamma = A_1,\ldots,A_n$ is their tensor product
\[
\den{\Gamma} = \den{A_1} \otimes \cdots \otimes \den{A_n}\,.
\]
If $\Gamma$ is empty then $\den{\Gamma} = k$.
\end{definition}

% We continue with the Church numeral $\church{2}_A$ as our motivating example:

%\begin{remark} This non-linearity can be traced to the duplication of terms that takes place during $\beta$-reduction. To see this, notice how in \eqref{eq:beta_reduc_dup} a single occurrence of $M$ on the left becomes a pair of $M$'s on the right, so that during the substitution process the term $M$ is duplicated. This duplication is not linear, because in general there is no linear map
%\begin{equation}\label{eq:dup_map_end}
%\End_k(V) \lto \End_k(V) \otimes \End_k(V)
%\end{equation}
%sending $\alpha$ to $\alpha \otimes \alpha$ for all $\alpha$, where $\End_k(V)$ denotes the space of linear maps $V \lto V$. One way to represent $T_{typed}$ as a linear map is to replace $\End_k(V)$ by the universal vector space over it which \emph{is} equipped with a duplication map satisfying some natural axioms; this is the universal cocommutative coalgebra ${!} \End_k(V)$. Using this coalgebra we may associate to $T_{typed}$ a linear map from which the non-linear map $\alpha \mapsto \alpha^2$ may be recovered; see Example \ref{example:denotation_2} below and Lemma \ref{lemma:nonlinear_recover}. %In the introduction we emphasised a link between duplication and infinity, and this will become clearer once we have understood the link between duplication and non-linearity, see Remark \ref{remark:dup_infinite} below.

%But we are getting ahead of ourselves. The first step is to reformulate the $\lambda$-calculus in terms of classical logic, then we identify the part of the logic that is responsible for this duplication -- called the contraction rule -- and explain how the contraction rule is refined in linear logic in such a way as to lead naturally to the coalgebra ${!} \End_k(V)$.
%\end{remark}

\begin{example}\label{example:denotation_2}  Let $A$ be a type whose denotation is $V = \den{A}$. Then from \eqref{defn:integers},
\[
\den{\inta_A} = \den{ {!}( A \multimap A) \multimap (A \multimap A)} = \Hom_k( {!} \End_k(V), \End_k(V) )\,.
\]
\end{example}

\subsection{Denotations of proofs}

In the previous section we associated to each type $A$ a vector space $\den{A}$. In this section we complete the construction of the vector space semantics of linear logic by assigning to each proof $\pi$ of a sequent $\Gamma \vdash B$ a linear map $\den{\Gamma} \lto \den{B}$. The essential features of this assignment can be seen already in the following simple example:

\begin{example} The denotation of the proof $\church{2}_A$ of $\vdash \inta_A$ will be a morphism
\begin{equation}\label{eq:2_as_comp_0}
\den{\church{2}_A}: k \lto \den{\inta_A} = \Hom_k( {!} \End_k(V), \End_k(V) )\,,
\end{equation}
or equivalently, a linear map ${!} \End_k(V) \lto \End_k(V)$. What is this linear map? It turns out (see Example \ref{example:church_2} below for details) that it is the composite
\begin{equation}\label{eq:2_as_comp}
\xymatrix@C+0.5pc{
{!} \End_k(V) \ar[r]^-{\Delta} & {!} \End_k(V) \otimes {!} \End_k(V) \ar[r]^-{d \otimes d} & \End_k(V)^{\otimes 2} \ar[r]^-{- \circ -} & \End_k(V)
}
\end{equation}
where $\Delta$ is the coproduct, $d$ is the universal map, and the last map is the composition. How to reconcile this linear map with the corresponding program in the $\lambda$-calculus, which has the meaning ``square the input function''? As we will explain below, for $\alpha \in \End_k( V )$ there is a naturally associated element $\ket{o}_\alpha \in {!} \End_k(V)$ with the property that
\[
\Delta \ket{o}_\alpha = \ket{o}_\alpha \otimes \ket{o}_\alpha, \qquad d \ket{o}_\alpha = \alpha\,.
\]
Then $\den{\church{2}_A}$ maps this element to
\begin{equation}\label{eq:2_deals_with_0}
\ket{o}_\alpha \longmapsto \ket{o}_\alpha \otimes \ket{o}_\alpha \longmapsto \alpha \otimes \alpha \longmapsto \alpha \circ \alpha\,.
\end{equation}
%For $n > 2$ the Church numeral $\church{n}$ has $n$ occurrences of the left $\multimap$ introduction rule, and so has as its leaves $n + 1$ uses of the axiom rule. The corresponding diagram uses the coproduct $n-1$ times, and feeds the results into $n$ derelictions.
This demonstrates how the coalgebra ${!} \End_k(V)$ may be used to encode nonlinear maps, such as squaring an endomorphism.
\end{example}

This is almost completely formal: the category of $k$-vector spaces has all the properties of a category of proof denotations described earlier, including the comonad ${!}$ given by taking cofree cocommutative coalgebras, so it is automatic that semantics of intuitionistic linear logic may be constructed within $\cat{V}$. However, to explicitly calculate the denotations of proofs we need formulas for promotion and dereliction which are not automatic: in fact, this paper seems to be the first time they have been written down. This is not as surprising as it might seem: although studying semantics of \emph{linear} logic using vector spaces is an natural thing to do, research has focused on full linear logic with negation. This is more complicated than the intuitionistic case, because types must be interpreted by self-dual objects and this leads to topological vector spaces \cite{barr_auto, barr_acc, blute,blute_scott,girard_coherentbanach,ehrhard, ehrhard_kothe}.  However these more sophisticated models have the disadvantage that it is inconvenient to write down denotations of proofs, which is why for this introduction we stick to the intuitionistic case. See Remark \ref{remark_otherpromotion} for more on the existing literature.

Our assignment of linear maps to proofs will be presented using string diagrams. One style of string diagrams, called \emph{proof-nets}, were introduced by Girard and have been fundamental to linear logic since the beginning of the subject \cite{girard_llogic}. However proof-nets are designed for full linear logic and this does not naturally fit with a semantics involving infinite-dimensional vector spaces. Instead we use a style of diagrams which is standard in category theory, following Joyal and Street \cite{JSGoTCI,JSGoTCII,ladia,khovdia}. Our recommended reference for this approach to proof-nets is Melli\`{e}s \cite{mellies, mellies_dia}.
\\

Let $\cat{V}$ denote the category of $k$-vector spaces (not necessarily finite dimensional). Then $\cat{V}$ is symmetric monoidal and for each object $V$ the functor $V \otimes -$ has a right adjoint
\[
V \multimap - := \Hom_k(V, -)\,.
\]
In addition to the usual diagrammatics of a symmetric monoidal category, we draw the counit $V \otimes (V \multimap W) \lto W$ as
\begin{center}
\begin{tabular}{>{\centering}m{5cm} >{\centering}m{1cm}}
\begin{tikzpicture}[scale=0.4,auto]
\node (topr) at (0,3) {$W$};
\coordinate (o) at (0,0) {};
\node (bottoml) at (-3,-4) {$V$};
\node (bottomr) at (3,-4) {$V \multimap W$};
\draw[out=90,in=180] (bottoml) to (o);
\draw[out=90,in=0] (bottomr) to (o);
\draw (o) to (topr);
\end{tikzpicture}
&
\tagarray{\label{eq:adjoint_map_diampee}}
\end{tabular}
\end{center}
The adjoint $Y \lto X \multimap Z$ of a morphism $\phi: X \otimes Y \lto Z$ is depicted as follows:\footnote{This is somewhat against the spirit of the diagrammatic calculus, since the loop labelled $X$ is not ``real'' and is only meant as a ``picture'' to be placed at a vertex between a strand labelled $Y$ and a strand labelled $X \multimap Z$. This should not cause confusion, because we will never manipulate this strand on its own. The idea is that if $X$ were a finite-dimensional vector space, so that $X \multimap Z \cong X^{\vee} \otimes Z$, the above diagram would be absolutely valid, and we persist with the same notation even when $X$ is not dualisable. In our judgement the clarity achieved by this slight cheat justifies a little valour in the face of correctness.}
\begin{center}
\begin{tabular}{>{\centering}m{5cm} >{\centering}m{1cm}}
\begin{tikzpicture}[scale=0.5,auto]
\node (topr) at (1,4) {$X \multimap Z$};
\mapnode (o) at (1,0) {};
\node [below] at (o.east) {$\phi$};
\node (gamma) at (2.5,-4) {$Y$};
\draw[out=90,in=0] (gamma) to (o);
\draw[out=0,in=180] (-1.5,-2) to node {$X$} (o);
\draw[out=180,in=270] (-1.5,-2) to (-3,0);
\draw[out=90,in=180] (-3,0) to (1,2);
\draw (o) to node [swap] {$Z$} (1,2);
\draw (1,2) to (topr);
\end{tikzpicture}
&
\tagarray{\label{eq:adjoint_map_diamp}}
\end{tabular}
\end{center}
Next we present the categorical construct corresponding to the exponential modality in terms of an adjunction, following Benton \cite{benton}, see also \cite[\S 7]{mellies}. Let $\cat{C}$ denote the category of counital, coassociative, cocommutative coalgebras in $\cat{V}$. In this paper whenever we say \emph{coalgebra} we mean an object of $\cat{C}$. This is a symmetric monoidal category in which the tensor product (inherited from $\cat{V}$) is cartesian, see \cite[Theorem 6.4.5]{sweedler}, \cite{barr} and \cite[\S 6.5]{mellies}. 

By results of Sweedler \cite[Chapter 6]{sweedler} the forgetful functor $L: \cat{C} \lto \cat{V}$ has a right adjoint $R$ and we set ${!} = L \circ R$, as in the following diagram:\footnote{The existence of a right adjoint to the forgetful functor can also be seen to hold more generally as a consequence of the adjoint functor theorem \cite{barr}.}
\[
\xymatrix@C+3pc{
\cat{C}\ar@<0.7ex>[r]^-{L} & \cat{V} \ar@<0.7ex>[l]^-{R}
}
\qquad
! = L \circ R\,.
\]
Both $L$ and its adjoint $R$ are monoidal functors.

For each $V$ there is a coalgebra $! V$ and a counit of adjunction $d: {!} V \lto V$. Since this map will end up being the interpretation of the dereliction rule in linear logic, we refer to it as the \emph{dereliction map}. In string diagrams it is represented by an empty circle. Although it is purely decorative, it is convenient to represent coalgebras in string diagrams drawn in $\cat{V}$ by thick lines, so that for ${!} V$ the dereliction, coproduct and counit are drawn respectively as follows:
\begin{center}
\begin{tabular}{>{\centering}m{3cm} >{\centering}m{3cm} >{\centering}m{3cm} >{\raggedleft}m{1cm}}
\begin{tikzpicture}[scale=0.5,auto,inner sep=1mm]
\node (top) at (0,2) {$V$};
\dernode (d) at (0,0) {};
\node (banga) at (0,-2) {$!V$};
\drawbang (banga) -- (d);
\draw (d) -- (top);
\end{tikzpicture}
&
\begin{tikzpicture}[scale=0.5,auto,inner sep=1mm]
\node (topl) at (-1,2) {${!} V$};
\node (topr) at (1,2) {${!} V$};
\coordinate (d) at (0,0);
\node (banga) at (0,-2) {$!V$};
\drawbang (banga) -- (d);
\drawbang[out=0,in=270] (d) to (topr);
\drawbang[out=180,in=270] (d) to (topl);
\end{tikzpicture}
&
\begin{tikzpicture}[scale=0.5,auto,inner sep=1mm]
\node (banga) at (0,-2) {$!V$};
\node[circle,draw=teal!50,fill=teal!50,inner sep=0.5mm] (blah) at (0,1) {};
\drawbang (banga) -- (blah);
\end{tikzpicture}
&
\tagarray{\label{eq:coalgebra_maps}}
\end{tabular}
\end{center}
In this paper our string diagrams involve both $\cat{V}$ and $\cat{C}$ and our convention is that white regions represent $\cat{V}$ and gray regions stand for $\cat{C}$. A standard way of representing monoidal functors between monoidal categories is using coloured regions \cite[\S 5.7]{mellies}. The image under $L$ of a morphism $\alpha: C_1 \lto C_2$ in $\cat{C}$ is drawn as a vertex in a grey region embedded into a white region. The image of a morphism $\gamma: V_1 \lto V_2$ under $R$ is drawn using a white region embedded in a gray plane. For example, the diagrams representing $L(\alpha), R(\gamma)$ and ${!} \gamma = LR(\gamma)$ are respectively
\begin{center}
\begin{tikzpicture}
[inner sep=0.5mm,scale=0.6,auto,place/.style={circle,draw=blue!50,fill=blue!20,thick},
transition/.style={circle,draw=black,fill=black}]
\node (vtop) at (0,4.5) {$L C_2$};
\node (bottom) at (0,-4.5) {$L C_1$};
\drawbang (top) to (vtop);
\coordinate (bottom_c) at (0,-3);
\drawbang (bottom) to (bottom_c);
\coordinate (top_c) at (0,3);
\draw[color=gray, line width=3pt, fill=gray!20] (0,0) ellipse (2cm and 3cm);
\node (o) at (0,0) [transition] {};
\node [right] at (o.east) {$\alpha$};
\draw (o) to node [swap] {$C_2$} (top_c);
\draw (bottom_c) to node [swap] {$C_1$} (o);
\draw[color=gray, line width=3pt] (0,0) ellipse (2cm and 3cm);
\end{tikzpicture}
\qquad \qquad
\begin{tikzpicture}
[inner sep=0.5mm,scale=0.6,auto,place/.style={circle,draw=blue!50,fill=blue!20,thick},
transition/.style={circle,draw=black,fill=black}]
\draw[color=white,fill=gray!20] (-3,-4.1) rectangle (3,4.1);
\node (vtop) at (0,4.5) {$R V_2$};
\node (bottom) at (0,-4.5) {$R V_1$};
\draw (top) to (vtop);
\coordinate (bottom_c) at (0,-3);
\draw (bottom) to (bottom_c);
\coordinate (top_c) at (0,3);
\draw[color=gray, line width=3pt, fill=white] (0,0) ellipse (2cm and 3cm);
\node (o) at (0,0) [transition] {};
\node [right] at (o.east) {$\gamma$};
\draw (o) to node [swap] {$V_2$} (top_c);
\draw (bottom_c) to node [swap] {$V_1$} (o);
\draw[color=gray, line width=3pt] (0,0) ellipse (2cm and 3cm);
\end{tikzpicture}
\qquad \qquad
\begin{tikzpicture}
[inner sep=0.5mm,scale=0.6,auto,place/.style={circle,draw=blue!50,fill=blue!20,thick},
transition/.style={circle,draw=black,fill=black}]
\node (vtop) at (0,4.5) {${!} V_2 = LR V_2$};
\node (bottom) at (0,-4.5) {${!} V_1 = LR V_1$};
\coordinate (top_c) at (0,3);
\coordinate (bottom_c) at (0,-3);
\drawbang (top) to (vtop);
\drawbang (bottom) to (bottom_c);
\draw[color=gray, line width=3pt, fill=gray!20] (0,0) ellipse (3cm and 3.2cm);
\draw[color=gray, line width=3pt, fill=white] (0,0) ellipse (2.5cm and 1.2cm);
\node (o) at (0,0) [transition] {};
\node [right] at (o.east) {$\gamma$};
\draw (o) to (0,1.2);
\draw (o) to (0,-1.2);
\draw (0,1.2) to node [swap] {$RV_2$} (0,3.2);
\draw (0,-3.2) to node [swap] {$RV_1$} (0,-1.2);
\draw[color=gray, line width=3pt] (0,0) ellipse (3cm and 3.2cm);
\draw[color=gray, line width=3pt] (0,0) ellipse (2.5cm and 1.2cm);
\end{tikzpicture}
\end{center}
The adjunction between $R$ and $L$ means that for any coalgebra $C$ and linear map $\phi: C \lto V$ there is a unique morphism of coalgebras $\Phi : C \lto {!}V$ making
\begin{equation}\label{eq:defining_philift}
\xymatrix@C+1.5pc@R+1.5pc{
C \ar[r]^-{\phi}\ar[dr]_-{\Phi} & V\\
& {!}V \ar[u]_-{d}
}
\end{equation}
commute. The lifting $\Phi$ may be constructed as the unit followed by ${!} \phi$,
\[
\Phi := \xymatrix@C+2pc{ C \ar[r] & {!} C \ar[r]^-{{!} \phi} & {!} V}
\]
and since we use an empty circle to denote the unit $C \lto {!} C$, this has the diagrammatic representation given on the right hand side of the following diagram. The left hand side is a convenient abbreviation for this morphism, that is, for the lifting $\Phi$:
% Promotion ======
\begin{center}
\begin{tabular}{>{\centering}m{3cm} >{\centering}m{1cm} >{\centering}m{5cm} m{1cm}}
\begin{tikzpicture}[scale=0.6,auto,inner sep=1mm]
\node (top) at (0,3) {$!V$};
\node (bottom) at (0,-3) {$C$};
\mapnode (o) at (0,0) {};
\node [right] at (o.east) {$\phi$};
\draw (o) to (0,2);
\drawbang (bottom) -- (0,-1.5);
\drawbang (0,1.5) -- (top);
\drawbang (0,-1.5) -- (o);
\drawprom (0,0) ellipse (1.5cm and 1.5cm);
\dernode (bottom) at (0,-1.5) {};
\end{tikzpicture}
& := &
\begin{tikzpicture}[scale=0.6,auto,inner sep=1mm]
\node (vtop) at (0,4.5) {${!} V$};
\node (bottom) at (0,-5.8) {$C$};
\coordinate (top_c) at (0,3);
\coordinate (bottom_c) at (0,-3.2);
\drawbang (top_c) to (vtop);
\dernode (bottomd) at (0,-4.5) {};
\drawbang (bottomd) to node [swap] {${!} C$} (bottom_c);
\drawbang (bottom) to (bottomd);
\draw[color=gray, line width=3pt, fill=gray!20] (0,0) ellipse (3cm and 3.2cm);
\draw[color=gray, line width=3pt, fill=white] (0,0) ellipse (2.5cm and 1.2cm);
\mapnode (o) at (0,0) {};
\node [right] at (o.east) {$\phi$};
\draw (o) to (0,1.2);
\drawbang (o) to (0,-1.1);
\draw (0,1.1) to node [swap] {$RV$} (0,3.1);
\draw (0,-3.1) to node [swap] {$RC$} (0,-1.3);
\end{tikzpicture}
& \tagarray{\label{eq:abbrev_for_prom}}
\end{tabular}
\end{center}
We follow the logic literature in referring to the grey circle denoting the induced map $\Phi$ as a \emph{promotion box}. Commutativity of \eqref{eq:defining_philift} is expressed by the identity
\begin{center}
\begin{tabular}{>{\centering}m{3cm} >{\centering}m{1cm} >{\centering}m{3cm} >{\raggedleft}m{1cm}}
\begin{tikzpicture}[scale=0.6,auto,inner sep=1mm]
\node (top) at (0,4) {$V$};
\dernode (altop) at (0,2.5) {};
\node (bottom) at (0,-3) {$C$};
\mapnode (o) at (0,0) {};
\node [right] at (o.east) {$\phi$};
\draw (o) to (0,2);
\drawbang (bottom) -- (0,-1.5);
\drawbang (0,1.5) -- (altop);
\draw (altop) -- (top);
\drawbang (0,-1.5) -- (o);
\drawprom (0,0) ellipse (1.5cm and 1.5cm);
\dernode (bottom) at (0,-1.5) {};
\end{tikzpicture}
& = &
\begin{tikzpicture}[scale=0.6,auto,inner sep=1mm]
\node (top) at (0,3) {$V$};
\node (bottom) at (0,-3) {$C$};
\mapnode (o) at (0,0) {};
\node [right] at (o.east) {$\phi$};
\drawbang (bottom) -- (o);
\draw (o) -- (top);
\end{tikzpicture}
& \tagarray{\label{eq:prom_cancel_der}}
\end{tabular}
\end{center}

The coalgebra ${!} V$ and the dereliction map ${!} V \lto V$ satisfy a universal property and are therefore unique up to isomorphism. However, to actually understand the denotations of proofs, we will need the more explicit construction which follows from the work of Sweedler \cite{sweedler} and is spelt out in \cite{murfet_coalg}. If $V$ is finite-dimensional then
\begin{equation}\label{eq:presentation_intro}
{!} V = \bigoplus_{P \in V} \Sym_P(V)
\end{equation}
where $\Sym_P(V) = \Sym(V)$ is the symmetric coalgebra. If $e_1,\ldots,e_n$ is a basis for $V$ then as a vector space $\Sym(V) \cong k[e_1,\ldots,e_n]$. The notational convention in \cite{murfet_coalg} is to denote, for elements $\nu_1,\ldots,\nu_s \in V$, the corresponding tensor in $\Sym_P(V)$ using kets
\begin{equation}\label{eq:ket_notation}
\ket{\nu_1,\ldots,\nu_s}_P := \nu_1 \otimes \cdots \otimes \nu_s \in \Sym_P(V)\,.
\end{equation}
And in particular, the identity element of $\Sym_P(V)$ is denoted by a vacuum vector
\begin{equation}\label{eq:vacuum}
\ket{o}_P := 1 \in \Sym_P(V)\,.
\end{equation}
We remark that if $\nu = 0$ then $\ket{\nu}_P = 0$ is the zero vector, which is distinct from $\ket{o}_P = 1$. We keep in mind that \eqref{eq:vacuum} denotes the case $s = 0$ of \eqref{eq:ket_notation} and to avoid unwieldy notation we sometimes write $\nu_1 \otimes \cdots \otimes \nu_s \cdot \ket{o}_P$ for $\ket{\nu_1,\ldots,\nu_s}_P$. With this notation the universal map $d: {!} V \lto V$ is defined by
\[
d \ket{o}_P = P, \quad d\ket{\nu}_P = \nu, \quad d\ket{\nu_1,\ldots,\nu_s}_P = 0 \quad s > 1\,.
\]
The coproduct on ${!} V$ is defined by
\begin{equation}\label{eq:lc_coalgebra_1}
\Delta \ket{ \nu_1, \ldots, \nu_s }_P = \sum_{I \subseteq \{ 1, \ldots, s \}} \ket{ \nu_I }_P \otimes \ket{ \nu_{I^c} }_P
\end{equation}
where $I$ ranges over all subsets including the empty set, for a subset $I = \{ i_1, \ldots, i_p \}$ we denote by $\nu_I$ the sequence $\nu_{i_1},\ldots,\nu_{i_p}$, and $I^c$ is the complement of $I$. In particular
\[
\Delta \ket{o}_P = \ket{o}_P \otimes \ket{o}_P\,.
\]
The counit ${!} V \lto k$ is defined by $\ket{o}_P \mapsto 1$ and $\ket{\nu_1,\ldots,\nu_s}_P \mapsto 0$ for $s > 0$.

When $V$ is infinite-dimensional we may define ${!} V$ as the direct limit over the coalgebras ${!} W$ for finite-dimensional subspaces $W \subseteq V$. These are sub-coalgebras of ${!} V$, and we may therefore use the same notation as in \eqref{eq:ket_notation} to denote arbitrary elements of ${!} V$. Moreover the coproduct, counit and universal map $d$ are given by the same formulas; see \cite[\S 2.1]{murfet_coalg}. A proof of the fact that the map $d: {!} V \lto V$ described above is universal among linear maps to $V$ from cocommutative coalgebras is given in \cite[Theorem 2.18]{murfet_coalg}, but as has been mentioned this is originally due to Sweedler \cite{sweedler}, see \cite[Appendix B]{murfet_coalg}.

To construct semantics of linear logic we also need an explicit description of liftings, as given by the next theorem which is \cite[Theorem 2.20]{murfet_coalg}. For a set $X$ the set of partitions of $X$ is denoted $\cat{P}_X$.

\begin{theorem}\label{theorem:describe_lifting} Let $W, V$ be vector spaces and $\phi: {!}W \lto V$ a linear map. The unique lifting to a morphism of coalgebras $\Phi: {!}W \lto {!}V$ is given by
\begin{equation}\label{eq:describe_lift}
\Phi \ket{ \nu_1, \ldots, \nu_s }_P = \sum_{C \in \cat{P}_{\{1,\ldots,s\}}} \phi \ket{ \nu_{C_{1}} }_P \otimes \cdots \otimes \phi \ket{ \nu_{C_{l}} }_P \cdot \ket{o}_Q
\end{equation}
for $P, \nu_1,\ldots,\nu_s \in W$, where $Q = \phi \ket{o}_P$ and $l$ denotes the length of the partition $C$.
\end{theorem}

%In constructing the vector space semantics of linear logic we will only encounter \eqref{eq:abbrev_for_prom} in the case where $C = {!} W$. %We will however give examples of lifting that involve other coalgebras, including the next example and in Appendix \ref{section:example_lifting}.

\begin{example}\label{example:lifting_trivial} The simplest example of a coalgebra is the field $k$. Any $P \in V$ determines a linear map $k \lto V$ whose lifting to a morphism of coalgebras $k \lto {!}V$ sends $1 \in k$ to the vacuum $\ket{o}_P$, as shown in the commutative diagram
\begin{equation}\label{eq:kaiwanxiao}
\xymatrix@C+2pc{
k \ar[r]^-{P} \ar@{.>}[dr]_-{\ket{o}_P} & V\\
& ! V \ar[u]_-{d}
}\,.
\end{equation}
Such liftings arise from promotions with empty premises, e.g. the proof\footnote{Incidentally, this proof helps explain why defining $\den{!A} = \Sym(\den{A})$ cannot lead to semantics of linear logic, since the denotation is a morphism of coalgebras $k \lto {!} \End_k(\den{A})$ whose composition with dereliction yields the map $k \lto \End_k( \den{A} )$ sending $1 \in k$ to the identity. But this map does not admit a lifting into the symmetric coalgebra, because it produces an infinite sum. However the symmetric coalgebra \emph{is} universal in a restricted sense and is (confusingly) sometimes also called a cofree coalgebra; see \cite[\S 4]{quillen}. For further discussion of the symmetric coalgebra in the context of linear logic see \cite{blute_fock,mellies2}.}
\begin{center}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\vdash A \multimap A$}
\RightLabel{\scriptsize prom}
\UnaryInfC{$\vdash {!}(A \multimap A)$}
\DisplayProof
\end{center}
\end{example}

\subsection{The vector space semantics}\label{section:vector_space_sem}

Recall from Definition \ref{defn:denotation_objects} the definition of $\den{A}$ for each type $A$.

\begin{definition}\label{defn:denotation_morphism} The \emph{denotation} $\den{\pi}$ of a proof $\pi$ of $\Gamma \vdash B$ is a linear map $\den{\Gamma} \lto \den{B}$ defined by inductively assigning a string diagram to each proof tree; by the basic results of the diagrammatic calculus \cite{JSGoTCI} this diagram unambiguously denotes a linear map. The inductive construction is described by the second column in \eqref{deduction_rule_ax} -- \eqref{deduction_rule_weak}. 

In each rule we assume a morphism has already been assigned to each of the sequents in the numerator of the deduction rule. These inputs are represented by blue circles in the diagram, which computes the morphism to be assigned to the denominator. To simplify the appearance of diagrams, we adopt the convention that a strand labelled by a type $A$ represents a strand labelled by the denotation $\den{A}$. In particular, a strand labelled with a sequence $\Gamma = A_1,\ldots,A_n$ represents a strand labelled by $\den{A_1} \otimes \cdots \otimes \den{A_n}$.
\end{definition}

Some comments:
\begin{itemize}
\item The diagram for the axiom rule \eqref{deduction_rule_ax} depicts the identity of $\den{A}$.
\item The diagram for the exchange rule \eqref{deduction_rule_ex} uses the symmetry $\den{B} \otimes \den{A} \lto \den{A} \otimes \den{B}$.
\item The diagram for the cut rule \eqref{deduction_rule_cut} depicts the composition of the two inputs.
\item The right tensor rule \eqref{deduction_rule_righttensor} depicts the tensor product of the two given morphisms, while the left tensor rule \eqref{deduction_rule_lefttensor} depicts the identity, viewed as a morphism between two strands labelled $\den{A}$ and $\den{B}$ and a single strand labelled $\den{A} \otimes \den{B}$.
\item The diagram for the right $\multimap$ rule \eqref{deduction_rule_righthom} denotes the adjoint of the input morphism, as explained in \eqref{eq:adjoint_map_diamp}, while the left $\multimap$ rule \eqref{deduction_rule_lefthom} uses the composition map of \eqref{eq:adjoint_map_diampee}.
\item The diagram for the promotion rule \eqref{deduction_rule_prom} depicts the lifting of the input to a morphism of coalgebras, as explained in \eqref{eq:abbrev_for_prom}.
\item The diagram for the dereliction rule \eqref{deduction_rule_der} depicts the composition of the input with the universal map out of the coalgebra ${!} V$. The notation for this map, and the maps in the contraction \eqref{deduction_rule_contr} and weakening rules \eqref{deduction_rule_weak} are as described in \eqref{eq:coalgebra_maps}.
\end{itemize}

\begin{remark} For this to be a valid semantics, two proofs related by cut-elimination must be assigned the same morphism. This is a consequence of the general considerations in \cite[\S 7]{mellies}. More precisely, $\cat{V}$ is a Lafont category \cite[\S 7.2]{mellies} and in the terminology of \emph{loc.cit.} the adjunction between $\cat{V}$ and $\cat{C}$ is a linear-nonlinear adjunction giving rise to a model of intuitionistic linear logic. For an explanation of how the structure of a symmetric monoidal category extrudes itself from the cut-elimination transformations, see \cite[\S 2]{mellies}.
\end{remark}

% Given the embedding of the simply-typed $\lambda$-calculus into linear logic, recalled in Appendix \ref{remark:embedding_intuit}, and the above construction of an interpretation of linear logic in the category $\cat{V}$ of $k$-vector spaces, we are finally in a position to answer Question \ref{question:lambda} in the positive. With patience, the reader may use the above to translate any program into a linear map. To explain how this works in practice, we go through the details of assigning a diagram and the corresponding linear map to the proof tree \eqref{church_2_prooftree} of the Church numeral $\church{2}_A$. 

\begin{example}\label{example:church_2} We convert the proof $\church{2}_A$ of \eqref{church_2_prooftree} to a diagram in stages, beginning with the leaves. Each stage is depicted in three columns: in the first is a partial proof tree, in the second is the diagram assigned to it by Definition \ref{defn:denotation_morphism}, and in the third is the explicit linear map which is the value of the diagram. 

Recall that a strand labelled $A$ actually stands for the vector space $V = \den{A}$, so for instance the first diagram denotes a linear map $V \otimes \End_k(V) \lto V$:
\begin{center}
% Church numeral 2 layer 1
\begin{tabular}{>{\centering}m{6cm} >{\centering}m{5cm} >{\centering}m{4cm}}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A \vdash A$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.3,auto]
\node (topr) at (0,2) {$A$};
\coordinate (o) at (0,0);
\node (delta) at (2.5,-4) {$A \multimap A$};
\node (left) at (-2.5, -4) {$A$};
\draw (o) to (topr);
\draw[out=90,in=180] (left) to (o);
\draw[out=90,in=0] (delta) to (o);
\end{tikzpicture}
&
$a \otimes \alpha \mapsto \alpha(a)$
\end{tabular}
\end{center}
\begin{center}
% Church numeral 2 layer 2
\begin{tabular}{>{\centering}m{6cm} >{\centering}m{5cm} >{\centering}m{4cm}}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A, A \multimap A \vdash A$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.3,auto]
\node (topr) at (1.5,2) {$A$};
\coordinate (q) at (-3,-4);
\coordinate (o) at (1.5,0);
\node (delta) at (6,-6) {$A \multimap A$};
\node (gamma) at (-3, -6) {$A$};
\node (ab) at (1,-6) {$A \multimap A$};
\draw[out=90,in=0] (delta) to (o);
\draw (o) to (topr);
\draw[out=90,in=180] (-1,-2) to (o);
\draw[out=90,in=270] (gamma) to (q);
\draw[out=90,in=180] (q) to (-1,-2);
\draw[out=90,in=0] (ab) to (-1,-2);
\end{tikzpicture}
&
$a \otimes \alpha \otimes \beta \mapsto \beta( \alpha(a) )$
\end{tabular}
\end{center}

\begin{center}
% Church numeral 2 layer 3
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{5cm} >{\centering}m{4cm}}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$A \multimap A, A \multimap A \vdash A \multimap A$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.3,auto]
\coordinate (o) at (2,0);
\node (top) at ($ (o) + (0,3) $) {$A \multimap A$}; % coordinates are relative to (o)

\coordinate (left_meet) at ($ (o) - (3, 2) $);
\draw[out=90,in=180] (left_meet) to (o);

% Coproduct and derelictions
\node (R) at ($ (o) + (3,-5) $) {$A \multimap A$};
\node (L) at ($ (o) + (-2,-5) $) {$A \multimap A$};
\coordinate (delta) at ($ (o) - (0,5) $);
\draw[out=90,in=0] (R) to (o);

% The A line curl from bottom to top
\coordinate (left_curve) at ($ (o) - (5, 4) $);
\coordinate (left_curve_mid) at ($ (o) - (6,2.5) $);
\coordinate (first_meeting_top) at ($ (o) + (0,1.5) $);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw (o) to (first_meeting_top);
\draw[out=180,in=270] (left_curve) to (left_curve_mid);
\draw[out=90,in=180] (left_curve_mid) to (first_meeting_top);

\draw (first_meeting_top) to (top);
\end{tikzpicture}
&
$\alpha \otimes \beta \mapsto \beta \circ \alpha$
\end{tabular}
\end{center}

\begin{center}
% TIKZ Church numeral 2 layer 4
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{1cm}}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$A \multimap A, A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize der}
\UnaryInfC{$!( A \multimap A ), A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize der}
\UnaryInfC{$!( A \multimap A ), !(A \multimap A) \vdash A \multimap A$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.3,auto,inner sep=1mm]
\coordinate (o) at (2,0);
\node (top) at ($ (o) + (0,3) $) {}; % coordinates are relative to (o)

\coordinate (left_meet) at ($ (o) - (3, 2) $);
\draw[out=90,in=180] (left_meet) to (o);

% Coproduct and derelictions
\dernode (R) at ($ (o) + (2,-4) $) {};
\dernode (L) at ($ (o) + (-2,-4) $) {};
\coordinate (delta) at ($ (o) - (0,5) $);
\draw[out=90,in=0] (R) to (o);
\drawbang (R) to ($ (R) - (0,2) $);
\drawbang (L) to ($ (L) - (0,2) $);

% The A line curl from bottom to top
\coordinate (left_curve) at ($ (o) - (5, 4) $);
\coordinate (left_curve_mid) at ($ (o) - (6,2.5) $);
\coordinate (first_meeting_top) at ($ (o) + (0,1.5) $);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw (o) to (first_meeting_top);
\draw[out=180,in=270] (left_curve) to (left_curve_mid);
\draw[out=90,in=180] (left_curve_mid) to (first_meeting_top);

\draw (first_meeting_top) to (top);
\end{tikzpicture}
&

\tagarray{\label{2_prime_pre}}

\end{tabular}
\end{center}
The map ${!} \End_k(V) \otimes {!} \End_k(V) \lto \End_k(V)$ in \eqref{2_prime_pre} is zero on $\ket{\nu_1,\ldots,\nu_s}_\alpha \otimes \ket{\mu_1,\ldots,\mu_t}_\beta$ for $\alpha,\beta \in \End_k(V)$ unless $s,t \le 1$, and in those cases it is given by
\begin{align*}
\ket{o}_\alpha \otimes \ket{o}_\beta &\longmapsto \beta \circ \alpha\\
\ket{\nu}_\alpha \otimes \ket{o}_\beta &\longmapsto \beta \circ \nu\\
\ket{o}_\alpha \otimes \ket{\mu}_\beta &\longmapsto \mu \circ \alpha\\
\ket{\nu}_\alpha \otimes \ket{\mu}_\beta &\longmapsto \mu \circ \nu\,.
\end{align*}
The next deduction rule in $\church{2}_A$ is a contraction:
\begin{center}
% TIKZ Church numeral 2 layer 4
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{6cm} >{\centering}m{1cm}}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$A \multimap A, A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize der}
\UnaryInfC{$!( A \multimap A ), A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize der}
\UnaryInfC{$!( A \multimap A ), !(A \multimap A) \vdash A \multimap A$}
\RightLabel{\scriptsize ctr}
\UnaryInfC{$!( A \multimap A ) \vdash A \multimap A$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.3,auto,inner sep=1mm]
\coordinate (o) at (2,0);
\node (top) at ($ (o) + (0,3) $) {}; % coordinates are relative to (o)

\coordinate (left_meet) at ($ (o) - (3, 2) $);
\draw[out=90,in=180] (left_meet) to (o);

% Coproduct and derelictions
\dernode (R) at ($ (o) + (2,-4) $) {};
\dernode (L) at ($ (o) + (-2,-4) $) {};
\coordinate (delta) at ($ (o) - (0,6) $);
\draw[out=90,in=0] (R) to (o);
\drawbang[out=270,in=0] (R) to (delta);
\drawbang[out=270,in=180] (L) to (delta);
\drawbang (delta) to ($ (delta) - (0,1.5) $);

% The A line curl from bottom to top
\coordinate (left_curve) at ($ (o) - (5, 4) $);
\coordinate (left_curve_mid) at ($ (o) - (6,2.5) $);
\coordinate (first_meeting_top) at ($ (o) + (0,1.5) $);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw (o) to (first_meeting_top);
\draw[out=180,in=270] (left_curve) to (left_curve_mid);
\draw[out=90,in=180] (left_curve_mid) to (first_meeting_top);

\draw (first_meeting_top) to (top);
\end{tikzpicture}

&

\tagarray{\label{2_prime}}
\end{tabular}
\end{center}
The denotation of this map is the composition $\phi = \den{\church{2}_A}: {!} \End_k(V) \lto \End_k(V)$ in \eqref{eq:2_as_comp}. We may compute using the above that 
\begin{equation}\label{eq:church_2_den}
\phi \ket{o}_\alpha = \alpha^2, \qquad \phi \ket{\nu}_\alpha = \{ \nu, \alpha \}\,, \qquad \phi \ket{\nu\mu}_\alpha = \{ \nu, \mu \}\,.
\end{equation}
For example
\begin{gather*}
\ket{\nu}_\alpha \longmapsto \ket{\nu}_\alpha \otimes \ket{o}_\alpha + \ket{o}_\alpha \otimes \ket{\nu}_\alpha \longmapsto \alpha \circ \nu + \nu \circ \alpha = \{ \nu, \alpha \}\,.
\end{gather*}
The final step in the proof of $\church{2}_A$ consists of moving the ${!}(A \multimap A)$ to the right side of the sequent, which yields the final diagram:
% TIKZ ----- Church numeral 2
\begin{center}
\begin{tabular}{ >{\centering}m{8cm} >{\centering}m{3cm}}
\begin{tikzpicture}[scale=0.55,auto]
\coordinate (o) at (2,0);
\node (top) at ($ (o) + (0,4) $) {$\inta_A$}; % coordinates are relative to (o)

\coordinate (left_meet) at ($ (o) - (3, 2) $);
\draw[out=90,in=180] (left_meet) to node {$A$} (o);

% Coproduct and derelictions
\dernode (R) at ($ (o) + (2,-3) $) {};
\dernode (L) at ($ (o) + (-2,-3) $) {};
\coordinate (delta) at ($ (o) - (0,5) $);
\draw[out=90,in=0] (R) to node [swap] {$A \multimap A$} (o);
\drawbang[out=0,in=270] (delta) to (R);
\drawbang[out=180,in=270] (delta) to (L);

% The A line curl from bottom to top
\coordinate (left_curve) at ($ (o) - (5, 4) $);
\coordinate (left_curve_mid) at ($ (o) - (6,2.5) $);
\coordinate (first_meeting_top) at ($ (o) + (0,1.5) $);
\draw[out=90,in=0] (L) to node [swap] {$A \multimap A$} (left_meet);
\draw[out=0,in=180] (left_curve) to node {$A$} (left_meet);
\draw (o) to node [swap] {$A$} (first_meeting_top);
\draw[out=180,in=270] (left_curve) to (left_curve_mid);
\draw[out=90,in=180] (left_curve_mid) to (first_meeting_top);

\coordinate (second_meeting_top) at ($ (first_meeting_top) + (0,1.5) $);
\draw (first_meeting_top) to node [swap] {$A \multimap A$} (second_meeting_top);
\draw (second_meeting_top) to (top);

\coordinate (curve_bottom) at ($ (left_meet) - (0,5) $);
\coordinate (curve_left) at ($ (o) - (7.5, 2) $);
\drawbang[out=0,in=270] (curve_bottom) to (delta);
\drawbang[out=180,in=270] (curve_bottom) to (curve_left);
\drawbang[out=90,in=180] (curve_left) to node {$!(A \multimap A)$} (second_meeting_top);
\end{tikzpicture}
&
\tagarray{\label{church_2_diagram}}
\end{tabular}
\end{center}
% End Church numeral 2
The denotation of this morphism is the map in \eqref{eq:2_as_comp_0}. The reader might like to compare this style of diagram for $\church{2}_A$ to the corresponding proof-net in \cite[\S 5.3.2]{girard_llogic}. 
\end{example}

In Example \ref{example:denotation_2} we sketched how to recover the function $\alpha \mapsto \alpha^2$ from the denotation of the Church numeral $\church{2}_A$, but now we can put this on a firmer footing. The translation of the $\lambda$-calculus into linear logic encourages us to think of a proof $\pi$ of a sequent $! B \vdash C$ as a program whose input of type $B$ may be used multiple times. There is \emph{a priori} no linear map $\den{B} \lto \den{C}$ associated to $\pi$ but there is a function $\den{\pi}_{nl}$ defined on $P \in \den{B}$ by lifting to $! \den{B}$ and then applying $\den{\pi}$:
\begin{equation}\label{eq:lifting_vacua}
\xymatrix@C+2pc{
k \ar[r]^-{P} \ar@{.>}[dr]_-{\ket{o}_P} & \den{B} & \den{C}\\
& ! \den{B} \ar[u]_-{d} \ar[ur]_-{\den{\pi}}
}\,.
\end{equation}
%We saw an example of this pattern in Example \ref{example:church_2}.
That is,

\begin{definition}\label{defn:nonlinear_denotation}
The function $\den{\pi}_{nl}: \den{B} \lto \den{C}$ is defined by $\den{\pi}_{nl}(P) = \den{\pi} \ket{o}_P$.
\end{definition}

The discussion above shows that, with $V = \den{A}$,

\begin{lemma}\label{lemma:nonlinear_recover} $\den{\church{2}_A}_{nl}: \End_k(V) \lto \End_k(V)$ is the map $\alpha \mapsto \alpha^2$.
\end{lemma}

This completes our explanation of how to represent the Church numeral $\church{2}_A$ as a linear map (modulo the evasion discussed in Remark \ref{remark_otherpromotion}). From this presentation we see clearly that the non-linearity of the map $\alpha \mapsto \alpha^2$ is concentrated, in fact, not in the duplication step but in the ``promotion'' step \eqref{eq:lifting_vacua} where the input vector $\alpha$ is turned into a vacuum $\ket{o}_\alpha \in {!} \End_k(V)$. The promotion step is non-linear since $\ket{o}_\alpha + \ket{o}_\beta$ is not a morphism of coalgebras and thus cannot be equal to $\ket{o}_{\alpha + \beta}$. After this step, the duplication, dereliction and composition shown in \eqref{eq:2_deals_with_0} are all linear. Finally, when $k = \mathbb{C}$ it is interesting to compare $\den{2}_{nl}$ with the map $\alpha \mapsto \alpha^2$ of smooth manifolds $\End_k(V)$, see Appendix \ref{section:example_lifting}.

%(see the discussion of tangents in Appendix \ref{section:example_lifting}) 

% Let $\church{2}_A'$ denote the proof of ${!}(A \multimap A) \vdash A \multimap A$ in \eqref{2_prime}. As discussed in Example \ref{example:denotation_2}, we may confuse the denotation of $\church{2}_A$ and $\church{2}_A'$. 
\begin{example}\label{example:2_promotion} Applied to $\church{2}_A$ the promotion rule generates a new proof
\begin{prooftree}
\AxiomC{$\church{2}_A$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!(A \multimap A) \vdash A \multimap A$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize prom}
\UnaryInfC{$!( A \multimap A) \vdash {!}(A \multimap A)$}
\end{prooftree}
which we denote $\operatorname{prom}( \church{2}_A )$. By definition the denotation $\Phi := \den{ \operatorname{prom}( \church{2}_A ) }$ of this proof is the unique morphism of coalgebras
\[
\Phi: {!} \End_k(V) \lto {!} \End_k(V)
\]
with the property that $d \circ \Phi = \phi$, where $\phi = \den{\church{2}_A}$ is as in \eqref{eq:2_as_comp}. From \eqref{eq:church_2_den} and Theorem \ref{theorem:describe_lifting} we compute that, for example
\begin{align*}
\Phi \ket{o}_\alpha &= \ket{o}_{\alpha^2}\,,\\
\Phi \ket{\nu}_\alpha &= \{ \nu, \alpha \} \cdot \ket{o}_{\alpha^2},\\
\Phi \ket{\nu \mu}_\alpha &= \big( \{ \nu, \mu \} + \{ \nu, \alpha \} \otimes \{ \mu, \alpha \} \big) \cdot \ket{o}_{\alpha^2}\,,\\
\Phi \ket{\nu \mu \theta}_\alpha &= \big( \{ \nu, \mu \} \otimes \{ \theta, \alpha \} + \{ \theta, \mu \} \otimes \{ \nu, \alpha \} + \{ \nu, \theta \} \otimes \{ \mu, \alpha \}\\
&+ \{ \nu, \alpha \} \otimes \{ \mu, \alpha \} \otimes \{ \theta, \alpha \} \big) \cdot \ket{o}_{\alpha^2}\,.
\end{align*}
Note that the commutators, e.g. $\{ \nu, \alpha \}$ are defined using the product internal to $\End_k(V)$, whereas inside the bracket in the last two lines, the terms $\{ \nu, \alpha \}$ and $\{ \mu, \alpha \}$ are multiplied in the algebra $\Sym( \End_k(V) )$ before being made to act on the vacuum.
\end{example}

%The approach we have chosen has the advantage of being universal in a certain sense: given another categorical semantics, say in a closed symmetric monoidal category $\cat{T}$ with comonad $Q$ modelling the exponential, if there is a strong monoidal functor $F: \cat{T} \lto \cat{V}$ (e.g. the objects of $\cat{T}$ are vector spaces with additional structure) then for any object $V \in \cat{T}$ the morphism $QV \lto V$ becomes a map $FQV \lto FV$, and since $FQV$ is a coalgebra, this factors as
%\[
%\xymatrix@C+1.5pc@R+1.5pc{
%FQV \ar[r]\ar[dr] & FV\\
%& {!}FV \ar[u]_-{d}
%}
%\]
%In this way it is possible to relate semantics such as Ehrhard's finiteness spaces \cite{ehrhard} to the ``universal'' semantics defined above.
\begin{remark}\label{remark_otherpromotion} As we have already mentioned, there are numerous other semantics of linear logic defined using topological vector spaces. The reader curious about how these vector space semantics are related to the ``relational'' style of semantics such as coherence spaces should consult Ehrhard's paper on finiteness spaces \cite{ehrhard}.

Indeed one can generate numerous examples of vector space semantics by looking at comonads on the category $\cat{V}$ defined by truncations on the coalgebras ${!} V$. For example, given a vector space $V$, let ${!}_0 V$ denote the subspace of ${!} V$ generated by the vacua $\ket{o}_P$. This is the free space on the underlying \emph{set} of $V$, and it is a subcoalgebra of ${!} V$ given as a coproduct of trivial coalgebras. It is easy to see that this defines an appropriate comonad and gives rise to a semantics of linear logic \cite[\S 4.3]{valiron}. However the semantics defined using ${!} V$ is more interesting, because universality allows us to mix in arbitrary coalgebras $C$. In Appendix \ref{section:example_lifting} we examine the simplest example where $C$ is the dual of the algebra $k[t]/t^2$ and relate this to tangent vectors.
\end{remark}

\section{Cut-elimination}\label{section:cut_elim}
%The formulas and proofs, to borrow an analogy from \cite[\S III]{girard_towards}, play within linear logic a role analogous to that of statics within classical mechanics. 

The dynamical part of linear logic is a rewrite rule on proofs called \emph{cut-elimination}, which defines the way that proofs interact. Collectively these interactions give the connectives their meaning. The full set of rewrite rules is given in \cite[Section 3]{mellies} and in the alternative language of proof-nets in \cite[\S 4]{girard_llogic}, \cite[p.18]{pagani}. While cut-elimination is the analogue in linear logic of $\beta$-reduction in the $\lambda$-calculus, the definition of the rewrite rules that make up cut-elimination is substantially more complicated; this is the price of consistently tracking the duplication of terms, which is ignored in the $\lambda$-calculus.

In order to help the reader grasp the core ideas without getting bogged down in the details, we present a worked example involving our favourite proof $\church{2}_A$ where we give both the rewrites and the associated transformations of string diagrams.

\begin{definition} Let $\Gamma \vdash A$ be a sequent of linear logic and $\cat{P}$ the set of proofs. There is a binary relation $\rightsquigarrow$ on $\cat{P}$ defined by saying that $\pi \rightsquigarrow \rho$ if $\rho$ is an immediate reduction of $\pi$ by one of the rewrite rules listed in \cite[Section 3]{mellies}. These rewrite rules are sometimes referred to as \emph{cut-elimination transformations}.

We write $=_{cut}$ for the smallest reflexive, transitive and symmetric relation on $\cat{P}$ containing $\rightsquigarrow$ and call this \emph{equivalence under cut-elimination}.
\end{definition}

%The rewrite rules listed in \cite[Section 3]{mellies}  %Conceptually, if $\pi \rightsquigarrow \rho$ then $\pi$ and $\rho$ contain the same logical content, but this content is expressed more explicitly in $\rho$. 

\begin{example}
In the context of Example \ref{example:cutagainst2} there is a sequence of transformations
\[
\church{2}_A \l \operatorname{prom}( \pi ) = \pi_0 \rightsquigarrow \pi_1 \rightsquigarrow \cdots \rightsquigarrow \pi \l \pi
\]
which begins with the rewrites \cite[\S 3.9.3]{mellies} and \cite[\S 3.9.1]{mellies}.
\end{example}

\begin{example}\label{example:cut_elim_examples} The cut-elimination transformation \cite[\S 3.8.2]{mellies} tells us that
\begin{center}
\begin{tabular}{ >{\centering}m{7cm} >{\centering}m{4cm} >{\centering}m{3cm}}
\AxiomC{$\pi_2$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$A \vdash B$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\vdash A \multimap B$}
\AxiomC{$\pi_1$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\Gamma \vdash A$}
\def\extraVskip{2pt}
\AxiomC{$\pi_3$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$B \vdash C$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize $\multimap L$}
\BinaryInfC{$\Gamma, A \multimap B \vdash C$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\Gamma \vdash C$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.35,auto]
\node (top) at (0,5) {$C$};
\mapnode (mid_top) at (0,2) {};
\node [above] at (mid_top.east) {$\;\;\;\;\;\;\den{\pi_3}$};
\node (bottoml) at (-3,-5) {$\Gamma$};
\mapnode (pi1) at (-3,-3) {};
\node [above] at (pi1.east) {$\;\;\;\;\;\;\;\;\;\den{\pi_1}$};
\coordinate (o) at (0,0);
\draw (o) -- (mid_top);
\draw (mid_top) -- (top);
\draw (bottoml) -- (pi1);
\draw[out=90,in=180] (pi1) to (o);
\mapnode (other) at (3,-3) {};
\coordinate (other_up) at (3,-2) {};
\node [right] at (other.east) {$\den{\pi_2}$};
\draw[out=90,in=0] (other_up) to (o);
\draw (other) to (other_up);
\coordinate (turn) at ($ (other) - (1.5,2) $);
\coordinate (upper_turn) at ($ (turn) + (-1,1) $);
\draw[out=270,in=0] (other) to (turn);
\draw[out=180,in=270] (turn) to (upper_turn);
\draw[out=90,in=180] (upper_turn) to (other_up);
\end{tikzpicture}
&
\tagarray{\label{eq:cutelim3}}
\end{tabular}
\end{center}
may be transformed to
\begin{center}
\begin{tabular}{ >{\centering}m{8cm} >{\centering}m{3cm} >{\centering}m{3cm}}
\AxiomC{$\pi_1$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\Gamma \vdash A$}
\def\extraVskip{2pt}
\AxiomC{$\pi_2$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$A \vdash B$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\Gamma \vdash B$}
\AxiomC{$\pi_3$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$B \vdash C$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\Gamma \vdash C$}
\DisplayProof
&
\begin{tikzpicture}[scale=0.35,auto,inner sep=1mm]
\node (top) at (0,5) {$C$};
\mapnode (pi3) at (0,3) {};
\node [right] at (pi3.east) {$\den{\pi_3}$};
\mapnode (pi2) at (0,1) {};
\node [right] at (pi2.east) {$\den{\pi_2}$};
\mapnode (pi1) at (0,-1) {};
\node [right] at (pi1.east) {$\den{\pi_1}$};
\node (bottom) at (0,-3) {$\Gamma$};
\draw (bottom) -- (pi1);
\draw (pi1) -- (pi2);
\draw (pi2) -- (pi3);
\draw (pi3) -- (top);
\end{tikzpicture}
&
\tagarray{\label{eq:cutelim4}}
\end{tabular}
\end{center}
Observe how this cut-elimination transformation encodes the meaning of the left introduction rule $\multimap L$, in the sense that it takes a proof $\pi_1$ of $\Gamma \vdash A$ and a proof $\pi_3$ of $B \vdash C$ and says: if you give me a proof of $A \vdash B$ I know how to sandwhich it between $\pi_1$ and $\pi_3$.
\end{example}

\begin{example}\label{example:cut_elim_examples2} The cut-elimination transformation \cite[\S 3.11.10]{mellies} tells us that
\begin{center}
\begin{tabular}{ >{\centering}m{7cm} >{\centering}m{4cm} >{\centering}m{3cm}}
\AxiomC{$\pi_1$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\Gamma \vdash A$}
\def\extraVskip{2pt}
\AxiomC{$\pi_2$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$B, A \vdash C$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$A \vdash B \multimap C$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\Gamma \vdash B \multimap C$}
\DisplayProof
&
\begin{tikzpicture}[inner sep = 0.5mm, scale=0.4,auto]
\node (topr) at (1,4) {$B \multimap C$};
\node[circle,draw=black,fill=black] (o) at (1,0) {};
\node [above] at (o.east) {$\;\;\;\;\;\;\;\;\den{\pi_2}$};
\node[circle,draw=black,fill=black] (other) at (2.5,-2) {};
\node [right] at (other.east) {$\den{\pi_1}$};
\node (gamma) at (2.5,-4) {$\Gamma$};
\draw[out=90,in=0] (gamma) to (o);
\draw[out=0,in=180] (-1.5,-2) to (o);
\draw[out=180,in=270] (-1.5,-2) to (-3,0);
\draw[out=90,in=180] (-3,0) to (1,2);
\draw (o) to (1,2);
\draw (1,2) to (topr);
\end{tikzpicture}
&
\tagarray{\label{eq:cutelim1}}
\end{tabular}
\end{center}
is transformed to the proof
\begin{center}
\begin{tabular}{ >{\centering}m{7cm} >{\centering}m{4cm} >{\centering}m{3cm}}
\AxiomC{$\pi_1$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\Gamma \vdash A$}
\def\extraVskip{2pt}
\AxiomC{$\pi_2$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$B, A \vdash C$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$B, \Gamma \vdash C$}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\Gamma \vdash B \multimap C$}
\DisplayProof
&
\begin{tikzpicture}[inner sep = 0.5mm, scale=0.4,auto]
\node (topr) at (1,4) {$B \multimap C$};
\node[circle,draw=black,fill=black] (o) at (1,0) {};
\node [above] at (o.east) {$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\den{\pi_2} \circ \den{\pi_1}$};
\node (gamma) at (2.5,-4) {$\Gamma$};
\draw[out=90,in=0] (gamma) to (o);
\draw[out=0,in=180] (-1.5,-2) to (o);
\draw[out=180,in=270] (-1.5,-2) to (-3,0);
\draw[out=90,in=180] (-3,0) to (1,2);
\draw (o) to (1,2);
\draw (1,2) to (topr);
\end{tikzpicture}
&
\tagarray{\label{eq:cutelim2}}
\end{tabular}
\end{center}
and thus the two corresponding diagrams are equal. In this case, the equality generated by cut-elimination expresses the fact that the Hom-tensor adjunction is natural.
\end{example}

\begin{example}\label{example:cut_elim_examples3} The cut-elimination transformation \cite[\S 3.6.1]{mellies} tells us that
\begin{center}
\begin{tabular}{ >{\centering}m{7cm} >{\centering}m{3cm}}
\AxiomC{} 
\UnaryInfC{$A \vdash A$}
\AxiomC{$\pi$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$A \vdash B$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$A \vdash B$}
\DisplayProof
&
\tagarray{\label{eq:cut_axiom_example}}
\end{tabular}
\end{center}
may be transformed to the proof $\pi$.
\end{example}

The analogue in linear logic of a normal form in $\lambda$-calculus is

\begin{definition} A proof is \emph{cut-free} if it contains no occurrences of the cut rule.
\end{definition}

The first theorem is that the relation $\rightsquigarrow$ is \emph{weakly normalising}, by which we mean:

\begin{theorem}[Cut-elimination] For each proof $\pi$ in linear logic there is a sequence
\begin{equation}\label{eq:proof_pi}
\pi = \pi_0 \rightsquigarrow \pi_1 \rightsquigarrow \cdots \rightsquigarrow \pi_n
\end{equation}
where $\pi_n$ is cut-free.
\end{theorem}
\begin{proof}
The original proof by Girard is given in the language of proof-nets \cite{girard_llogic} which for various reasons are better suited to the problem. For a proof in the language of sequent calculus see for example \cite[Appendix B]{brauner}. The argument follows Gentzen's proof for cut-elimination in classical logic; see \cite{gentzen} and \cite[Chapter 13]{girard_prooftypes}. 
\end{proof}

This is the analogue of Gentzen's \emph{Hauptsatz} for ordinary logic, which was mentioned in the introduction. At a high level, the sequence of rewrites \eqref{eq:proof_pi} eliminates the implicitness present in the original proof until the fully explicit, cut-free proof $\pi_n$ is left. In practice there are various kinds of cut-elimination transformations, for example:
\begin{itemize}
\item those like \eqref{eq:cutelim1} $\rightsquigarrow$ \eqref{eq:cutelim2} in Example \ref{example:cut_elim_examples} which commute cuts past other deduction rules or otherwise rearrange the content of the proof.
\item those like \eqref{eq:cutelim3} $\rightsquigarrow$ \eqref{eq:cutelim4} in Example \ref{example:cut_elim_examples2} which replace a cut on a complex formula $A \multimap B$ by cuts on simpler formulae $A,B$. 
\item those like the transformation in Example \ref{example:cut_elim_examples3} which actually erase part of the proof.
% With some abuse of language (to be discussed in a moment) we will refer below to $\pi_n$ as the \emph{cut-free normalisation} of $\pi$.
\end{itemize}

\begin{remark} It is possible that multiple rewrite rules apply to a proof, so that there is more than one sequence \eqref{eq:proof_pi} with $\pi$ as its starting point. This raises the question of whether the end result $\pi_n$ is independent of the choices at each stage. This property of a rewrite rule is called \emph{confluence}. Unfortunately in its sequent calculus presentation linear logic is \emph{not} confluent and in particular it is not true that each cut-elimination equivalence class contains a unique cut-free proof.

However it is true that two cut-free proofs in the same equivalence class are ``essentially the same'' in the sense that they are assigned the same proof-net. That is, cut-elimination of proof-nets is confluent, according to \cite[Remark 4.19]{pagani}. The reader is warned that in this direction things quickly become technical!
\end{remark}

\subsection{An extended example}

To demonstrate cut-elimination in a nontrivial example, let us prove that $2 \times 2 = 4$.

\begin{definition}\label{definition:mult_m} We define $\prf{\mathrm{mult}}_A(m, -)$ to be the proof (writing $E = A \multimap A$)
\begin{center}
% TIKZ Church numeral 2 layer 4
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{4cm}}
\AxiomC{$\church{m}_A$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{${!}E \vdash E$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize prom}
\UnaryInfC{${!}E \vdash {!}E$}
\AxiomC{}
\UnaryInfC{$E \vdash E$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{${!} E, \inta_A \vdash E$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$\inta_A \vdash \inta_A$}
\DisplayProof

&

\tagarray{\label{cut_mult_2_0}}
\end{tabular}
\end{center}
\end{definition}

This proof represents multiplication by $m$, in the following sense.

\begin{lemma} The proof obtained from $\prf{\mathrm{mult}}_A(m,-)$ by cutting against $\church{n}_A$
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{4cm}}
\AxiomC{$\church{n}_A$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash \inta_A$}
\def\extraVskip{2pt}
\AxiomC{$\prf{\mathrm{mult}}_A(m,-)$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\inta_A \vdash \inta_A$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\vdash \inta_A$}
\DisplayProof
&

\tagarray{\label{cut_mult_prooftree}}
\end{tabular}
\end{center}
is equivalent under cut-elimination to $\church{mn}_A$.
\end{lemma}

Rather than give a complete proof of this lemma, we examine the special case where $m = n = 2$. If we denote the proof in \eqref{cut_mult_prooftree} by $\prf{\mathrm{mult}}_A(2,-) \l \church{2}_A$ then its string diagrams is
\begin{center}
\begin{tabular}{ >{\centering}m{6cm} >{\centering}m{4cm}}
% TIKZ ---- picture of mult_2
\begin{tikzpicture}[scale=0.35,auto,inner sep=1mm]
\coordinate (topr) at (0,2);
\coordinate (comp) at (3,4);
\drawbang[out=90,in=180] (topr) to (comp);
\drawprom (0,-1) ellipse (3cm and 3cm);
\dernode (bottomr) at (0,-4) {};

% -------- promoted 2 ---------
% All coordinates are relative to the composition vertex (o), which itself
% has its coordinates derived from topr. This is the only link to the outside namespace.
\coordinate (o) at ($ (topr) + (1,-2) $);
\coordinate (ellipse_center) at ($ (o) - (1,1) $);
\coordinate (elbow) at ($ (o) - (3, 0.5) $);
\coordinate (top_of_circle) at ($ (o) + (-1,2) $);
\draw[out=90,in=270] (o) to (top_of_circle);
\drawprom (ellipse_center) ellipse (3cm and 3cm); % outer boundary
\dernode (bottomr) at ($ (o) - (1,4) $) {};
\dernode (R) at ($ (o) + (1, -1.5) $) {}; % right dereliction
\dernode (L) at ($ (o) - (1,1.5) $) {}; % left dereliction
\coordinate (left_curve) at ($ (o) - (2.5,2) $);
\coordinate (left_meet) at ($ (o) - (1.5, 0.9) $);
\coordinate (delta) at ($ (o) - (0, 2.7) $); % coproduct vertex
\draw[out=90,in=0] (R) to node [swap] {} (o);
\drawbang[out=0,in=270] (delta) to (R);
\drawbang[out=180,in=270] (delta) to (L);
\drawbang[out=90,in=270] (bottomr) to (delta);
\draw[out=90,in=180] (left_meet) to (o);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw[out=180,in=270] (left_curve) to (elbow);
\draw[out=90,in=180] (elbow) to ($ (o)!.5!(top_of_circle) $);
% ------ end promoted 2 ------

% Now for the rest of the diagram
\coordinate (curve) at (-3.9, -3);
\coordinate (meet) at (3,5.5);
\drawbang[out=300,in=270] (curve) to (bottomr);
\drawbang[out=120,in=180] (curve) to (meet);
\node (vtop) at (3,7) {$\inta_A$};
\draw (meet) to (vtop);
\draw (comp) to (meet);

% ----------The Church 2 on the right
\coordinate (2o) at ($ (topr) + (8,-2) $);

\coordinate (2left_meet) at ($ (2o) - (1.5, 0.9) $);
\draw[out=90,in=180] (2left_meet) to (2o);

% Coproduct and derelictions
\dernode (2R) at ($ (2o) + (1,-1.5) $) {};
\dernode (2L) at ($ (2o) + (-1,-1.5) $) {};
\coordinate (2delta) at ($ (2o) - (0,2.7) $);
\draw[out=90,in=0] (2R) to (2o);
\drawbang[out=0,in=270] (2delta) to (2R);
\drawbang[out=180,in=270] (2delta) to (2L);

% The A line curl from bottom to top
\coordinate (2left_curve) at ($ (2o) - (2.5, 2) $);
\coordinate (2left_curve_mid) at ($ (2o) - (3,1.25) $);
\coordinate (2first_meeting_top) at ($ (2o) + (0,0.75) $);
\draw[out=90,in=0] (2L) to (2left_meet);
\draw[out=0,in=180] (2left_curve) to (2left_meet);
\draw[out=180,in=270] (2left_curve) to (2left_curve_mid);

\coordinate (2curve_bottom) at ($ (2left_meet) - (0,2.5) $);
\coordinate (2curve_left) at ($ (2o) - (3.75, 1) $);
\drawbang[out=0,in=270] (2curve_bottom) to (2delta);
\drawbang[out=180,in=270] (2curve_bottom) to (2curve_left);


% the lines from the right 2 to the comp vertex are scaled along the line
\coordinate (2top) at ($ (2o) + (0,1.5) $);
\draw[out=90,in=0] (2top) to (comp);
\draw (2o) to (2top);
\drawbang[out=90,in=180] (2curve_left) to (2top);
\draw[out=90,in=180] (2left_curve_mid) to ($ (2o)!.5!(2top) $);
\end{tikzpicture}

&

\tagarray{\label{cut_mult_2}}
\end{tabular}
\end{center}
To prove that the cut-free normalisation of $\prf{\mathrm{mult}}_A(2,-) \l \church{2}_A$ is the Church numeral $\church{4}_A$ we run through the proof transformations generated by the cut-elimination algorithm, each of which yields a new proof with the same denotation. This sequence of proofs represents a particular sequence of manipulations of the string diagram. 

To \eqref{cut_mult_2} we apply naturality of the Hom-tensor adjunction, as in \eqref{eq:cutelim1} $\rightsquigarrow$ \eqref{eq:cutelim2}. Then we are in the position of \eqref{eq:cutelim3}, with $\pi_2$ a part of $\church{2}_A$ and $\pi_1$ the promoted Church numeral. The step \eqref{eq:cutelim3} $\rightsquigarrow$ \eqref{eq:cutelim4} is to take the left leg and feed it as an input to the right leg. This yields the first equality below. The second equality follows from the fact that a promotion box represents a morphism of coalgebras, and thus can be commuted past the coproduct whereby it is duplicated:
\begin{center}
% TIKZ picture ---- stacked 2 promotions LEFT
\begin{tabular}{m{0.8cm} m{0.3cm} m{4cm} m{0.3cm} m{6cm} m{1cm}}
\eqref{cut_mult_2} & = &
\begin{tikzpicture}[scale=0.4,auto,inner sep=1mm]
\coordinate (topr) at (0,2); % top of the bottom promoted 2
\coordinate (curve) at (-4, -2.9);
%\coordinate (comp) at (0,4) {};
\node (vtop) at ($ (topr) + (0,8) $) {};
\coordinate (meet) at ($ (vtop) - (0,2) $); % where the curved !( A -o A ) arc meets at the top

\draw (meet) to (vtop);

% ----------The Church 2 on the right
\coordinate (2o) at ($ (topr) + (0,4) $);
\coordinate (2left_meet) at ($ (2o) - (1.5, 0.9) $);
\draw[out=90,in=180] (2left_meet) to (2o);

% Coproduct and derelictions
\dernode (2R) at ($ (2o) + (1,-1.5) $) {};
\dernode (2L) at ($ (2o) + (-1,-1.5) $) {};
\coordinate (2delta) at ($ (2o) - (0,2.7) $);
\draw[out=90,in=0] (2R) to (2o);
\drawbang[out=0,in=270] (2delta) to (2R);
\drawbang[out=180,in=270] (2delta) to (2L);

% The A line curl from bottom to top
\coordinate (2left_curve) at ($ (2o) - (2.5, 2) $);
\coordinate (2left_curve_mid) at ($ (2o) - (3,1.25) $);
\coordinate (2first_meeting_top) at ($ (2o) + (0,0.75) $);
\draw[out=90,in=0] (2L) to (2left_meet);
\draw[out=0,in=180] (2left_curve) to (2left_meet);
\draw[out=180,in=270] (2left_curve) to (2left_curve_mid);
\draw[out=90,in=180] (2left_curve_mid) to (2first_meeting_top);
\draw (2o) to (meet);

%\draw[out=90,in=270] (2join_at_top) to (meet);
\drawbang (topr) to (2delta);

% -------- promoted 2 ---------
% All coordinates are relative to the composition vertex (o), which itself
% has its coordinates derived from topr. This is the only link to the outside namespace.
\coordinate (o) at ($ (topr) + (1,-2) $);
\coordinate (ellipse_center) at ($ (o) - (1,1) $);
\coordinate (elbow) at ($ (o) - (3, 0.5) $);
\coordinate (top_of_circle) at ($ (o) + (-1,2) $);
\draw[out=90,in=270] (o) to (top_of_circle);
\drawprom (ellipse_center) ellipse (3cm and 3cm); % outer boundary
\dernode (bottomr) at ($ (o) - (1,4) $) {};
\dernode (R) at ($ (o) + (1, -1.5) $) {}; % right dereliction
\dernode (L) at ($ (o) - (1,1.5) $) {}; % left dereliction
\coordinate (left_curve) at ($ (o) - (2.5,2) $);
\coordinate (left_meet) at ($ (o) - (1.5, 0.9) $);
\coordinate (delta) at ($ (o) - (0, 2.7) $); % coproduct vertex
\draw[out=90,in=0] (R) to node [swap] {} (o);
\drawbang[out=0,in=270] (delta) to (R);
\drawbang[out=180,in=270] (delta) to (L);
\drawbang[out=90,in=270] (bottomr) to (delta);
\draw[out=90,in=180] (left_meet) to (o);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw[out=180,in=270] (left_curve) to (elbow);
\draw[out=90,in=180] (elbow) to ($ (o)!.5!(top_of_circle) $);
% ------ end promoted 2 ------

% The loop from the bottom of the promoted 2 up to the top
\drawbang[out=300,in=270] (curve) to (bottomr);
\drawbang[out=120,in=180] (curve) to (meet);
\end{tikzpicture}

& = &

% ---------------------- TIKZ stacked 2's, RIGHT
\begin{tikzpicture}[scale=0.4,auto,inner sep=1mm]
\coordinate (topr) at (0,2); % top of the bottom promoted 2
%\coordinate (comp) at (0,4) {};
\node (vtop) at ($ (topr) + (0,8) $) {};
\coordinate (meet) at ($ (vtop) - (0,2) $); % where the curved !( A -o A ) arc meets at the top

\draw (meet) to (vtop);

% ----------The Church 2 on top
\coordinate (2o) at ($ (topr) + (0,4) $);
\coordinate (2left_meet) at ($ (2o) - (1.5, 0.9) $);
\draw[out=90,in=180] (2left_meet) to (2o);

% Coproduct and derelictions
\dernode (2R) at ($ (2o) + (1,-1.5) $) {};
\dernode (2L) at ($ (2o) + (-1,-1.5) $) {};
\draw[out=90,in=0] (2R) to (2o);

% The A line curl from bottom to top
\coordinate (2left_curve) at ($ (2o) - (2.5, 2) $);
\coordinate (2left_curve_mid) at ($ (2o) - (3,1.25) $);
\coordinate (2first_meeting_top) at ($ (2o) + (0,0.75) $);
\draw[out=90,in=0] (2L) to (2left_meet);
\draw[out=0,in=180] (2left_curve) to (2left_meet);
\draw[out=180,in=270] (2left_curve) to (2left_curve_mid);
\draw[out=90,in=180] (2left_curve_mid) to (2first_meeting_top);
\draw (2o) to (meet);

%\draw[out=90,in=270] (2join_at_top) to (meet);
\drawbang[out=90,in=270] ($ (topr) + (-3.5,0) $) to (2L);
\drawbang[out=90,in=270] ($ (topr) + (3.5,0) $) to (2R);

% -------- promoted 2 LEFT ---------
% All coordinates are relative to the composition vertex (o), which itself
% has its coordinates derived from topr. This is the only link to the outside namespace.
\coordinate (o) at ($ (topr) + (-2.5,-2) $);
\coordinate (ellipse_center) at ($ (o) - (1,1) $);
\coordinate (elbow) at ($ (o) - (3, 0.5) $);
\coordinate (top_of_circle) at ($ (o) + (-1,2) $);
\draw[out=90,in=270] (o) to (top_of_circle);
\drawprom (ellipse_center) ellipse (3cm and 3cm); % outer boundary
\dernode (bottomr) at ($ (o) - (1,4) $) {};
\dernode (R) at ($ (o) + (1, -1.5) $) {}; % right dereliction
\dernode (L) at ($ (o) - (1,1.5) $) {}; % left dereliction
\coordinate (left_curve) at ($ (o) - (2.5,2) $);
\coordinate (left_meet) at ($ (o) - (1.5, 0.9) $);
\coordinate (delta) at ($ (o) - (0, 2.7) $); % coproduct vertex
\draw[out=90,in=0] (R) to node [swap] {} (o);
\drawbang[out=0,in=270] (delta) to (R);
\drawbang[out=180,in=270] (delta) to (L);
\drawbang[out=90,in=270] (bottomr) to (delta);
\draw[out=90,in=180] (left_meet) to (o);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw[out=180,in=270] (left_curve) to (elbow);
\draw[out=90,in=180] (elbow) to ($ (o)!.5!(top_of_circle) $);
% ------ end promoted 2 ------

% -------- promoted 2 RIGHT ---------
% All coordinates are relative to the composition vertex (o), which itself
% has its coordinates derived from topr. This is the only link to the outside namespace.
\coordinate (o) at ($ (topr) + (4.5,-2) $);
\coordinate (ellipse_center) at ($ (o) - (1,1) $);
\coordinate (elbow) at ($ (o) - (3, 0.5) $);
\coordinate (top_of_circle) at ($ (o) + (-1,2) $);
\draw[out=90,in=270] (o) to (top_of_circle);
\drawprom (ellipse_center) ellipse (3cm and 3cm); % outer boundary
\dernode (right_bottomr) at ($ (o) - (1,4) $) {};
\dernode (R) at ($ (o) + (1, -1.5) $) {}; % right dereliction
\dernode (L) at ($ (o) - (1,1.5) $) {}; % left dereliction
\coordinate (left_curve) at ($ (o) - (2.5,2) $);
\coordinate (left_meet) at ($ (o) - (1.5, 0.9) $);
\coordinate (delta) at ($ (o) - (0, 2.7) $); % coproduct vertex
\draw[out=90,in=0] (R) to node [swap] {} (o);
\drawbang[out=0,in=270] (delta) to (R);
\drawbang[out=180,in=270] (delta) to (L);
\drawbang[out=90,in=270] (right_bottomr) to (delta);
\draw[out=90,in=180] (left_meet) to (o);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw[out=180,in=270] (left_curve) to (elbow);
\draw[out=90,in=180] (elbow) to ($ (o)!.5!(top_of_circle) $);
% ------ end promoted 2 ------


% The loop from the bottom of the promoted 2 up to the top
\coordinate (curve) at (-8, 0);
\coordinate (very_bottom_turn) at (-2,-7);
\coordinate (very_bottom_delta) at (0,-6);
\drawbang[out=270,in=180] (curve) to (very_bottom_turn);
\drawbang[out=90,in=180] (curve) to (meet);
\drawbang[out=0,in=270] (very_bottom_turn) to (very_bottom_delta);
\drawbang[out=0,in=270] (very_bottom_delta) to (right_bottomr);
\drawbang[out=180,in=270] (very_bottom_delta) to (bottomr);
\end{tikzpicture}

&

\tagarray{\label{diagram_mult_2_2}}

\end{tabular}
\end{center}
At this point the promotions cancel with the derelictions by the identity \eqref{eq:prom_cancel_der}, ``releasing'' the pair of Church numerals contained in the promotion boxes. This yields the first equality below, while the second is an application of the general form of \eqref{eq:cutelim3} $\rightsquigarrow$ \eqref{eq:cutelim4}:
\begin{center}
\begin{tabular}{m{0.8cm} m{0.5cm} m{4.5cm} m{0.5cm} m{5cm}}
\eqref{diagram_mult_2_2} & = &
% ---------------------- TIKZ stacked 2's, RIGHT
\begin{tikzpicture}[scale=0.3,auto,inner sep=1mm]
\coordinate (topr) at (0,2); % top of the bottom promoted 2
%\coordinate (comp) at (0,4) {};
\node (vtop) at ($ (topr) + (0,8) $) {};
\coordinate (meet) at ($ (vtop) - (0,2) $); % where the curved !( A -o A ) arc meets at the top

\draw (meet) to (vtop);

% ----------The Church 2 on top
\coordinate (2o) at ($ (topr) + (0,4) $);
\coordinate (2left_meet) at ($ (2o) - (1.5, 0.9) $);
\draw[out=90,in=180] (2left_meet) to (2o);

% Coproduct and derelictions
\coordinate (2R) at ($ (2o) + (1,-1.5) $);
\coordinate (2L) at ($ (2o) + (-1,-1.5) $);
\draw[out=90,in=0] (2R) to (2o);

% The A line curl from bottom to top
\coordinate (2left_curve) at ($ (2o) - (2.5, 2) $);
\coordinate (2left_curve_mid) at ($ (2o) - (3,1.25) $);
\coordinate (2first_meeting_top) at ($ (2o) + (0,0.75) $);
\draw[out=90,in=0] (2L) to (2left_meet);
\draw[out=0,in=180] (2left_curve) to (2left_meet);
\draw[out=180,in=270] (2left_curve) to (2left_curve_mid);
\draw[out=90,in=180] (2left_curve_mid) to (2first_meeting_top);
\draw (2o) to (meet);

%\draw[out=90,in=270] (2join_at_top) to (meet);
\draw[out=90,in=270] ($ (topr) + (-3.5,0) $) to (2L);
\draw[out=90,in=270] ($ (topr) + (3.5,0) $) to (2R);

% -------- promoted 2 LEFT ---------
% All coordinates are relative to the composition vertex (o), which itself
% has its coordinates derived from topr. This is the only link to the outside namespace.
\coordinate (o) at ($ (topr) + (-2.5,-2) $);
\coordinate (elbow) at ($ (o) - (3, 0.5) $);
\coordinate (top_of_circle) at ($ (o) + (-1,2) $);
\dernode (R) at ($ (o) + (1, -1.8) $) {}; % right dereliction
\dernode (L) at ($ (o) - (1,1.8) $) {}; % left dereliction
\coordinate (left_curve) at ($ (o) - (2.5,2) $);
\coordinate (left_meet) at ($ (o) - (1.5, 0.9) $);
\coordinate (delta) at ($ (o) - (0, 3) $); % coproduct vertex
\draw[out=90,in=0] (R) to node [swap] {} (o);
\drawbang[out=0,in=270] (delta) to (R);
\drawbang[out=180,in=270] (delta) to (L);
\draw[out=90,in=180] (left_meet) to (o);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw[out=180,in=270] (left_curve) to (elbow);
\draw[out=90,in=270] (o) to (top_of_circle);
\draw[out=90,in=180] (elbow) to ($ (o)!.5!(top_of_circle) $);
% ------ end promoted 2 ------

% -------- promoted 2 RIGHT ---------
% All coordinates are relative to the composition vertex (o), which itself
% has its coordinates derived from topr. This is the only link to the outside namespace.
\coordinate (o) at ($ (topr) + (4.5,-2) $);
\coordinate (elbow) at ($ (o) - (3, 0.5) $);
\coordinate (top_of_circle) at ($ (o) + (-1,2) $);
\dernode (R) at ($ (o) + (1, -1.8) $) {}; % right dereliction
\dernode (L) at ($ (o) - (1,1.8) $) {}; % left dereliction
\coordinate (left_curve) at ($ (o) - (2.5,2) $);
\coordinate (left_meet) at ($ (o) - (1.5, 0.9) $);
\coordinate (right_delta) at ($ (o) - (0, 3) $); % coproduct vertex
\draw[out=90,in=0] (R) to node [swap] {} (o);
\drawbang[out=0,in=270] (right_delta) to (R);
\drawbang[out=180,in=270] (right_delta) to (L);
\draw[out=90,in=180] (left_meet) to (o);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw[out=180,in=270] (left_curve) to (elbow);
\draw[out=90,in=270] (o) to (top_of_circle);
\draw[out=90,in=180] (elbow) to ($ (o)!.5!(top_of_circle) $);
% ------ end promoted 2 ------


% The loop from the bottom of the promoted 2 up to the top
\coordinate (curve) at (-8, 0);
\coordinate (very_bottom_turn) at (-2,-7);
\coordinate (very_bottom_delta) at (1,-5);
\drawbang[out=270,in=180] (curve) to (very_bottom_turn);
\drawbang[out=90,in=180] (curve) to (meet);
\drawbang[out=0,in=270] (very_bottom_turn) to (very_bottom_delta);
\drawbang[out=0,in=270] (very_bottom_delta) to (right_delta);
\drawbang[out=180,in=270] (very_bottom_delta) to (delta);
\end{tikzpicture}

& = &

% Final TikZ image for Church 4

\begin{tikzpicture}[scale=0.3,auto,inner sep=1mm]
\coordinate (topr) at (0,2); % top of the bottom promoted 2
%\coordinate (comp) at (0,4) {};
\node (vtop) at ($ (topr) + (0,8) $) {};
\coordinate (meet) at ($ (vtop) - (0,2) $); % where the curved !( A -o A ) arc meets at the top

\draw (meet) to (vtop);

% -------- promoted 2 LEFT ---------
% All coordinates are relative to the composition vertex (o), which itself
% has its coordinates derived from topr. This is the only link to the outside namespace.
\coordinate (o) at ($ (topr) + (-2.5,-2) $);
\coordinate (elbow) at ($ (o) - (3, 0.5) $);
\coordinate (top_of_circle) at ($ (o) + (0,1) $);
\dernode (R) at ($ (o) + (1, -1.8) $) {}; % right dereliction
\dernode (L) at ($ (o) - (1,1.8) $) {}; % left dereliction
\coordinate (left_curve) at ($ (o) - (2.5,2) $);
\coordinate (left_meet) at ($ (o) - (1.5, 0.9) $);
\coordinate (delta) at ($ (o) - (0, 3) $); % coproduct vertex
\draw[out=90,in=0] (R) to node [swap] {} (o);
\drawbang[out=0,in=270] (delta) to (R);
\drawbang[out=180,in=270] (delta) to (L);
\draw[out=90,in=180] (left_meet) to (o);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw[out=180,in=270] (left_curve) to (elbow);
\draw[out=90,in=270] (o) to (top_of_circle);
\draw[out=90,in=180] (elbow) to (top_of_circle);
% ------ end promoted 2 ------

% -------- promoted 2 RIGHT ---------
% All coordinates are relative to the composition vertex (o), which itself
% has its coordinates derived from topr. This is the only link to the outside namespace.
\coordinate (o) at ($ (topr) + (2.5,3) $);
\coordinate (elbow) at ($ (o) - (3, 0.5) $);
\coordinate (right_top_of_circle) at ($ (o) + (-1,2) $);
\dernode (R) at ($ (o) + (1, -1.8) $) {}; % right dereliction
\dernode (L) at ($ (o) - (1,1.8) $) {}; % left dereliction
\coordinate (left_curve) at ($ (o) - (2.5,2) $);
\coordinate (left_meet) at ($ (o) - (1.5, 0.9) $);
\coordinate (right_delta) at ($ (o) - (0, 3) $); % coproduct vertex
\draw[out=90,in=0] (R) to node [swap] {} (o);
\drawbang[out=0,in=270] (right_delta) to (R);
\drawbang[out=180,in=270] (right_delta) to (L);
\draw[out=90,in=180] (left_meet) to (o);
\draw[out=90,in=0] (L) to (left_meet);
% ------ end promoted 2 ------

\draw[out=90,in=180] (top_of_circle) to (left_meet);
\draw[out=90,in=270] (o) to (meet);

% The loop from the bottom of the promoted 2 up to the top
\coordinate (curve) at (-8, 0);
\coordinate (very_bottom_turn) at (-2,-7);
\coordinate (very_bottom_delta) at (0,-5);
\drawbang[out=270,in=180] (curve) to (very_bottom_turn);
\drawbang[out=90,in=180] (curve) to (meet);
\drawbang[out=0,in=270] (very_bottom_turn) to (very_bottom_delta);
\drawbang[out=0,in=270] (very_bottom_delta) to (right_delta);
\drawbang[out=180,in=270] (very_bottom_delta) to (delta);
\end{tikzpicture}
\end{tabular}
\end{center}
This last diagram is the denotation of $\church{4}_A$, so we conclude that (at least at the level of the denotations) the output of the program $\prf{\mathrm{mult}}_A(2,-)$ on the input $\church{2}_A$ is $\church{4}_A$.

\section{Second-order linear logic}\label{section:second}

The propositional linear logic introduced in the previous sections is not very ``expressive'' in the sense that only a limited range of functions on integers may be encoded as proofs. To remedy this we need to add quantifiers, and the resulting language is referred to as \emph{second-order} intuitionistic linear logic. The corresponding semantics is much less clear than the propositional case, so we will not speak about it here; see however \cite{??,??}. 

Our aim in this section is to begin by writing down some arithmetic in the propositional part of linear logic, and then to show how iteration on higher types leads naturally to the need for quantifiers in order to properly represent towers of exponentials. Recall that the type of integers on $A$ is $\inta_A = {!}(A \multimap A) \multimap (A \multimap A)$. %Throughout $A$ is any formula and we write $\church{n}_A$ for the Church numeral $n$ viewed as a proof of $\inta_A$.

The following examples are taken from \cite[\S 5.3.2]{girard_llogic} and \cite[\S 4]{danos}.

\begin{definition} We define $\prf{\mathrm{comp}}_A$ to be the proof 
\begin{center}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$A \multimap A, A \multimap A \vdash A \multimap A$}
\DisplayProof
\qquad
\tagarray{\label{proof_comp}}
\end{center}
As discussed in Remark \ref{example:church_2} the denotation of this proof is the function which \emph{composes} two endomorphisms of  $\den{A}$.
\end{definition}

\begin{example}[(Addition)] We define $\prf{\mathrm{add}}_A$ to be the proof (writing $E = A \multimap A$)
\begin{center}
\AxiomC{}
\UnaryInfC{${!}E \vdash {!}E$}
\AxiomC{}
\UnaryInfC{${!}E \vdash {!}E$}
\AxiomC{$\prf{\mathrm{comp}}_A$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$E, E \vdash E$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{${!}E, E, \inta_A \vdash E$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{${!} E, {!} E, \inta_A, \inta_A \vdash E$}
\RightLabel{\scriptsize ctr}
\UnaryInfC{${!} E, \inta_A, \inta_A \vdash E$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$\inta_A, \inta_A \vdash \inta_A$}
\DisplayProof
\qquad
\tagarray{\label{add_prooftree}}
\end{center}
which encodes addition on $A$-integers in the following sense: if $\prf{\mathrm{add}}_A$ is cut against two proofs $\church{m}_A$ and $\church{n}_A$ the resulting proof is equivalent under cut-elimination to $\church{m+n}_A$. 

Let us call the result of the cut $\prf{\mathrm{add}}_A( m, n )$, which is a proof of $\inta_A$. One way to see that this reduces to $\underline{m+n}_A$ without laboriously performing cut-elimination by hand is to use a term calculus, as presented in for example \cite{abramsky, benton_etal} or \cite[\S 4]{danos}, to understand the computational content of the proof. Alternatively, with the vector space semantics in mind, one can understand the proof as follows: given a vacuum $\ket{o}_\alpha$ at an endomorphism $\alpha$ of $V = \den{A}$, the contraction step duplicates this to $\ket{o}_\alpha \otimes \ket{o}_\alpha$. The left hand copy of $\ket{o}_\alpha$ is fed into $\underline{m}_A$ yielding $\alpha^m$ and the right hand copy is fed into $\underline{n}_A$ yielding $\alpha^n$. Then the composition ``machine'' composes these outputs to yield $\alpha^{m+n}$.
\end{example}

To generate a hierarchy of increasingly complex proofs from addition and multiplication we employ \emph{iteration}.

\begin{example}[(Iteration)]\label{example:proof_iteration} Given a proof $\beta$ of $A$ and $\beta'$ of $A \multimap A$ we define $\prf{\mathrm{rec}}_A(\beta, \beta')$ to be the proof
\begin{center}
\AxiomC{$\beta'$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash A \multimap A$}
\RightLabel{\scriptsize prom}
\UnaryInfC{$\vdash {!}(A \multimap A)$}
\def\extraVskip{2pt}
\AxiomC{$\beta$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$\inta_A \vdash A$}
\DisplayProof
\qquad
\tagarray{\label{iteration_prooftree}}
\end{center}
Cut against $\church{n}_A$ this yields the $n$th power of $\beta'$ applied to $\beta$. By analogy with induction in ordinary mathematical argument, often $\beta'$ is referred to as the \emph{step function} and $\beta$ as the \emph{base case}.
\end{example}

This game gets more interesting when we get to exponentials. From Definition \ref{definition:mult_m} we know how to define a proof $\prf{\mathrm{mult}}_A(m,-)$ whose cut against $\church{n}_A$ yields something equivalent under cut-elimination to $\church{mn}_A$. But how do we construct a proof which, when cut against $\church{n}_A$, yields a proof equivalent to $\church{m^n}_A$? Naturally we can iterate multiplication by $m$, but the catch is that this requires integers of type $\inta_{\inta_A}$.

\begin{example}[(Exponentials)]\label{example:exponentials} We define $\prf{\mathrm{exp}}_{A,m}$ to be
\[
\prf{\mathrm{exp}}_{A,m} = \prf{\mathrm{rec}}_{\inta_A}\big( \church{1}_{\inta_A}, \prf{\mathrm{mult}}_A(m,-) \big)\,.
\]
That is,
\begin{center}
\AxiomC{$\prf{\mathrm{mult}}_A(m,-)$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash \inta_A \multimap \inta_A$}
\RightLabel{\scriptsize prom}
\UnaryInfC{$\vdash {!}(\inta_A \multimap \inta_A)$}
\def\extraVskip{2pt}
\AxiomC{$\church{1}_{\inta_A}$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash \inta_A$}
\AxiomC{}
\UnaryInfC{$\inta_A \vdash \inta_A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$\inta_A \multimap \inta_A \vdash \inta_A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$\inta_{\inta_A} \vdash \inta_A$}
\DisplayProof
\qquad
\tagarray{\label{exp_prooftree}}
\end{center}
Cut against $\church{n}_{\inta_A}$ this yields the desired numeral $\church{m^n}_A$.
\end{example}

We begin to see the general pattern: to represent more complicated functions $\mathbb{N} \lto \mathbb{N}$ we need to use more complicated base types $B$ for the integers $\inta_B$ on the left hand side. At the next step, when we try to iterate exponentials, we see that it is hopeless to continue without introducing a way to parametrise over these base types. That is precisely the role of quantifiers in second-order logic.

The iteration of the function $n \mapsto 2^n$ yields a tower of exponentials of variable height. More precisely, $T: \mathbb{N} \lto \mathbb{N}$ is defined recursively by $T(0) = 1, T(n+1) = 2^{T(n)}$ so that
\[
T(n) = 2^{2^{2^{\iddots^{2}}}}
\]
where the total number of occurrences of $2$ on the right hand side is $n$. To represent the function $T$ as a proof in linear logic we need to introduce integer types with iterated subscripts by the recursive definition
\[
\inta_A(0) = \inta_A, \qquad \inta_A(r+1) =\inta_{\inta_A(r)}\,.
\]
From Example \ref{example:exponentials} we have, for each $r$, the proof $\prf{\mathrm{exp}}_{\inta_A(r),2}$ of $\inta_A(r+1) \vdash \inta_A(r)$. By iteration of the exponential we mean the cut of the following proofs:
\begin{center}
\begin{tabular}{ >{\centering}m{3cm} >{\centering}m{2cm} >{\centering}m{3cm} >{\centering}m{3cm} >{\centering}m{2cm}}
\AxiomC{$\prf{\mathrm{exp}}_{\inta_A(n),2}$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\inta_A(n+1) \vdash \inta_A(n)$}
\DisplayProof

&

$\cdots$

&

\AxiomC{$\prf{\mathrm{exp}}_{\inta_{\inta_A},2}$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\inta_{\inta_{\inta_A}} \vdash \inta_{\inta_A}$}
\DisplayProof

&

\AxiomC{$\prf{\mathrm{exp}}_{\inta_A,2}$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\inta_{\inta_A} \vdash \inta_A$}
\DisplayProof

&
\tagarray{\label{seriesofexponentials}}
\end{tabular}
\end{center}
The result of these cuts is a proof of $\inta_A(n+1) \vdash \inta_A$ which, when cut against $\church{n}_{\inta_A(n)}$, yields a proof which reduces under cut-elimination to $\church{T(n)}_A$.

However this proof is constructed ``by hand'' for each integer $n$. It is clear from the definition of \eqref{seriesofexponentials} that what we are doing in essence is iterating the exponential function, but we cannot express this formally in the language of propositional linear logic because the base type on our integers changes from iteration to iteration. This leads us to second-order linear logic. After defining it, we will return to the problem of encoding towers of exponentials as proofs.

\begin{definition}[(Second-order linear logic)] The formulas of \emph{second-order linear logic} are defined recursively as follows: any formula of propositional linear logic is a formula, and if $A$ is a formula then so is $\forall x \,.\, A$ for any propositional variable $x$. There are two new deduction rules:
\begin{center}
\begin{tabular}{ >{\centering}m{4cm} >{\centering}m{4cm} }
\AxiomC{$\Gamma \vdash A$}
\RightLabel{\scriptsize$\forall R$}
\UnaryInfC{$\Gamma \vdash \forall x \,.\, A$}
\DisplayProof

&

\AxiomC{$\Gamma, A[B/x] \vdash C$}
\RightLabel{\scriptsize$\forall L$}
\UnaryInfC{$\Gamma, \forall x \,.\, A \vdash C$}
\DisplayProof
\end{tabular}
\end{center}
where $\Gamma$ is a sequence of formulas, possibly empty, and in the right introduction rule we require that $x$ is not free in any formula of $\Gamma$. Here $A[B/x]$ means a formula $A$ with all free occurrences of $x$ replaced by a formula $B$ (as usual, there is some chicanery necessary to avoid free variables in $B$ being captured, but we ignore this).
\end{definition}

Intuitively, the right introduction rule takes a proof $\pi$ and ``exposes'' the variable $x$, for which any type may be substituted. The left introduction rule, dually, takes a formula $B$ in some proof and binds it to a variable $x$. The result of cutting a right introduction rule against a left introduction rule is that the formula $B$ will be bound to $x$ throughout the proof $\pi$. That is, cut-elimination transforms
\begin{center}
\AxiomC{$\pi$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\Gamma \vdash A$}
\RightLabel{\scriptsize $\forall R$}
\UnaryInfC{$\Gamma \vdash \forall x \,.\, A$}
\def\extraVskip{2pt}
\AxiomC{$\rho$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\Delta, A[B/x] \vdash C$}
\RightLabel{\scriptsize $\forall L$}
\UnaryInfC{$\Delta, \forall x \,.\, A \vdash C$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\Gamma, \Delta \vdash C$}
\DisplayProof
\qquad
\tagarray{\label{quantifier_before}}
\end{center}
to the proof
\begin{center}
\AxiomC{$\pi[B/x]$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\Gamma \vdash A[B/x]$}
\def\extraVskip{2pt}
\AxiomC{$\rho$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\Delta, A[B/x] \vdash C$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\Gamma, \Delta \vdash C$}
\DisplayProof
\qquad
\tagarray{\label{quantifier_before}}
\end{center}
where $\pi[B/x]$ denotes the result of replacing all occurrences of $x$ in the proof $\pi$ with $B$. In the remainder of this section we provide a taste of just what an \emph{explosion} of possibilities the addition of quantifiers to the language represents. 

\begin{example}[(Integers)] The type of integers is
\[
\inta = \forall x \,.\, {!}(x \multimap x) \multimap (x \multimap x)\,.
\]
For each integer $n \ge 0$ we define $\church{n}$ to be the proof
\begin{center}
\AxiomC{$\inta_x$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash \inta_x$}
\RightLabel{\scriptsize$\forall R$}
\UnaryInfC{$\vdash \inta$}
\DisplayProof
\end{center}
\end{example}

% Notice that for any formula $B$, we have $\inta_B = \inta_x[B/x]$, so that if we have a proof with hypothesis $\inta_B$ we can apply the left introduction rule for $\forall$ to convert this to $\inta$.

\begin{example}[(Exponentials)] We define $\prf{\mathrm{exp}}_m$ to be
\begin{center}
\AxiomC{$\prf{\mathrm{exp}}_{x,m}$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\inta_{\inta_x} \vdash \inta_x$}
\RightLabel{\scriptsize$\forall L$}
\UnaryInfC{$\inta \vdash \inta_x$}
\RightLabel{\scriptsize$\forall R$}
\UnaryInfC{$\inta \vdash \inta$}
\DisplayProof
\end{center}
% Notice that the left introduction rule must precede the right introduction rule, because of the requirement that $x$ not be free on the left hand side in the latter rule.
\end{example}

\begin{example}[(Hyper-exponentials)] We define $\prf{\mathrm{hypexp}}$ to be the iteration of $\prf{\mathrm{exp}}_2$, in the following sense:
\begin{center}
\AxiomC{$\prf{\mathrm{exp}}_2$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash \inta \multimap \inta$}
\RightLabel{\scriptsize prom}
\UnaryInfC{$\vdash {!}(\inta \multimap \inta)$}
\def\extraVskip{2pt}
\AxiomC{$\church{1}$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash \inta$}
\AxiomC{}
\UnaryInfC{$\inta \vdash \inta$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$\inta \multimap \inta \vdash \inta$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$\inta_{\inta} \vdash \inta$}
\RightLabel{\scriptsize $\forall L$}
\UnaryInfC{$\inta \vdash \inta$}
\DisplayProof
\qquad
\tagarray{\label{hyperexp_prooftree}}
\end{center}
\end{example}

\begin{lemma} The cut of $\church{n}$ against $\prf{\mathrm{hypexp}}$ reduces to $\church{T(n)}$.
\end{lemma}

Once we have added quantifiers, the expressive power of the language is immense. We can for example easily iterate hyper-exponentials to obtain a tower of exponentials whose height is itself a tower of exponentials, and by iterating at the type $\inta \multimap \inta$ obtain monsters like the Ackermann function \cite[\S 6.D]{girard_blindspot}.

\begin{theorem} Any function $\mathbb{N} \lto \mathbb{N}$ which is provably total in second-order Peano arithmetic can be encoded into second-order linear logic as a proof of $\inta \vdash \inta$.
\end{theorem}

\appendix

\section{Example of cut-elimination}

We examine the beginning of the cut-elimination process applied to the proof \eqref{cut_mult_2}. Our reference for cut-elimination is Melli\`{e}s \cite[\S 3.3]{mellies}. Throughout a formula $A$ is fixed and we write $E = A \multimap A$ so that $\inta_A = {!}E \multimap E$. We encourage the reader to put the following series of proof trees side-by-side with the evolving diagrams in the above to see the correspondence between cut-elimination and diagram manipulation.

To begin, we expose the first layer of structure within $\prf{\mathrm{mult}}_A(2,-)$ to obtain
\begin{center}
\AxiomC{$\church{2}_A$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash \inta_A$}
\def\extraVskip{2pt}
\AxiomC{$\prf{\mathrm{mult}}_A(2,-)$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E, \inta_A \vdash E$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\inta_A \vdash \inta_A$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\vdash \inta_A$}
\DisplayProof
\qquad
\tagarray{\label{cut_step_1}}
%\begin{tikzpicture}[scale=0.6,auto]
%\node (topr) at (1,4) {$\inta_A$};
%\bluenode (o) at (1,0) {};
%\bluenode (gam) at (2.5,-2) {};
%\draw[out=90,in=0] (gam) to node [swap]{$\inta_A$} (o);
%\drawbang[out=0,in=180] (-1.5,-2) to node {} (o);
%\drawbang[out=180,in=270] (-1.5,-2) to (-3,0);
%\drawbang[out=90,in=180] (-3,0) to (1,2);
%\draw (o) to node [swap] {$E$} (1,2);
%\draw (1,2) to (topr);
%\end{tikzpicture}
%\end{tabular}
\end{center}
For a cut against a proof whose last deduction rule is a right introduction rule for $\multimap$, the cut elimination procedure \cite[\S 3.11.10]{mellies} prescribes that \eqref{cut_step_1} be transformed to
\begin{center}
\AxiomC{$\church{2}_A$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash \inta_A$}
\def\extraVskip{2pt}
\AxiomC{$\prf{\mathrm{mult}}_A(2,-)$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E, \inta_A \vdash E$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$!E \vdash E$}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\vdash \inta_A$}
\DisplayProof
\qquad
\tagarray{\label{cut_step_2}}
\end{center}
If we fill in the content of $\prf{\mathrm{mult}}_A(2,-)$, this proof may be depicted as follows:
\begin{center}
\AxiomC{$\church{2}_A'$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E  \vdash E$}
\def\extraVskip{2pt}
\UnaryInfC{$\vdash \inta_A$}
\AxiomC{$\church{2}_A'$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E \vdash E$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize prom}
\UnaryInfC{$!E \vdash {!}E$}
\AxiomC{}
\UnaryInfC{$E \vdash E$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$!E, \inta_A \vdash E$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$!E \vdash E$}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\vdash \inta_A$}
\DisplayProof
\qquad
\tagarray{\label{cut_step_3}}
\end{center}
The next cut-elimination step \cite[\S 3.8.2]{mellies} transforms this proof to
\begin{center}
\AxiomC{$\church{2}_A'$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E \vdash E$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize prom}
\UnaryInfC{$!E \vdash {!}E$}
\AxiomC{$\church{2}_A'$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E \vdash E$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$!E \vdash E$}
\AxiomC{}
\UnaryInfC{$E \vdash E$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$!E \vdash E$}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\vdash \inta_A$}
\DisplayProof
\quad
\tagarray{\label{cut_step_4}}
\end{center}
As may be expected, cutting against an axiom rule does nothing, so this is equivalent to
\begin{prooftree}
\AxiomC{$\church{2}_A'$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E \vdash E$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize prom}
\UnaryInfC{$!E \vdash {!}E$}
\AxiomC{$\church{2}_A''$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E, !E \vdash E$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize ctr}
\UnaryInfC{$!E \vdash E$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$!E \vdash E$}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\vdash \inta_A$}
\end{prooftree}
where $\church{2}_A''$ is a sub-proof of $\church{2}_A$. Here is the important step: cut-elimination replaces a cut of a promotion against a contraction by a pair of promotions \cite[\S 3.9.3]{mellies}. This step corresponds to the doubling of the promotion box in \eqref{diagram_mult_2_2}
\begin{prooftree}
\AxiomC{$\church{2}_A'$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E \vdash E$}
\def\extraVskip{2pt}
\UnaryInfC{$!E \vdash {!}E$}
\AxiomC{$\church{2}_A'$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E \vdash E$}
\def\extraVskip{2pt}
\UnaryInfC{$!E \vdash {!}E$}
\AxiomC{$\church{2}_A''$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!E, !E \vdash E$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$!E, !E \vdash E$}
\RightLabel{\scriptsize cut}
\BinaryInfC{$!E, !E \vdash E$}
\RightLabel{\scriptsize ctr}
\UnaryInfC{$!E \vdash E$}
\RightLabel{\scriptsize $\multimap R$}
\UnaryInfC{$\vdash \inta_A$}
\end{prooftree}
We only sketch the rest of the cut-elimination process: next, the derelictions in $\church{2}_A''$ will be annihilate with the promotions in the two copies of $\church{2}_A'$ according to \cite[\S 3.9.1]{mellies}. Then there are numerous eliminations involving the right and left $\multimap$ introduction rules. %, which are not very interesting. %The point is that after the next step, cuts on formulas $!(A \multimap A)$ will be replaced by cuts on formulas $A \multimap A$, which are of lower complexity. 

\section{Tangents and proofs}\label{section:example_lifting}

%In this appendix we consider various examples of the lifting formula in Theorem \ref{theorem:describe_lifting}, centered around the notion of tangent vectors at proof denotations. 


%\begin{definition}\label{defn:nonlinear_deno} Given a proof $\pi$ of a sequent $!A \vdash B$ let $\langle \pi \rangle$ denote the function
%\begin{gather*}
%\langle \pi \rangle: \den{A} \lto \den{B},\\
%\langle \pi \rangle(P) = \den{\pi} \ket{o}_P\,.
%\end{gather*}
%\end{definition}

%One interesting example of lifting involves tangent vectors at proofs.

\begin{example}\label{example:tangent_coalgebra} Let $\cat{T}$ denote the dual of the finite-dimensional algebra $k[t]/(t^2)$. It has a $k$-basis $1 = 1^*$ and $\varepsilon = t^*$ and coproduct $\Delta$ and counit $u$ defined by
\[
\Delta(1) = 1 \otimes 1, \quad \Delta( \varepsilon ) = 1 \otimes \varepsilon + \varepsilon \otimes 1, \quad u(1) = 1, \quad u(\varepsilon) = 0\,.
\]
Recall that a tangent vector at a point $x$ on a scheme $X$ is a morphism $\Spec(k[t]/t^2) \lto X$ sending the closed point to $x$. Given a finite-dimensional vector space $V$ and $R = \Sym(V^*)$ with $X = \Spec(R)$, this is equivalent to a morphism of $k$-algebras
\[
\varphi: \Sym(V^*) \lto k[t]/t^2
\]
with $\varphi^{-1}( (t) ) = x$. Such a morphism of algebras is determined by its restriction to $V^*$, which as a linear map $\varphi|_{V^*}: V^* \lto k[t]/t^2$ corresponds to a pair of elements $(P, Q)$ of $V$, where $\varphi( \tau ) = \tau(P) \cdot 1 + \tau(Q) \cdot t$. Then $\varphi$ sends a polynomial $f$ to
\[
\varphi(f) = f(P) \cdot 1 + \partial_Q( f )|_P \cdot t\,.
\]
The map $\varphi|_{V^*}$ is also determined by its dual, which is a linear map $\phi: \cat{T} \lto V$. By the universal property, this lifts to a morphism of coalgebras $\Phi: \cat{T} \lto {!}V$. If $\phi$ is determined by a pair of points $(P,Q) \in V^{\oplus 2}$ as above, then it may checked directly that
\[
\Phi( 1 ) = \ket{o}_P, \qquad \Phi( \varepsilon ) = \ket{ Q }_P
\]
is a morphism of coalgebras lifting $\phi$.
%Using the dual of $k[t]/t^N$ for various powers $N$ we may in this way make an identification between ${!} V$ and the infinite jet-space of $V$.
\end{example}

Motivated by this example, we make a preliminary investigation into tangent vectors at proof denotations. Let $A,B$ be types with finite-dimensional denotations $\den{A}, \den{B}$.

\begin{definition} Given a proof $\pi$ of $\vdash A$ a \emph{tangent vector} at $\pi$ is a morphism of coalgebras $\theta: \cat{T} \lto {!} \den{A}$ with the property that $\theta(1) = \ket{o}_{\den{\pi}}$, or equivalently that the diagram
\begin{equation}
\xymatrix@C+2pc{
k \ar[d]_-{1} \ar[r]^-{\den{\pi}} & \den{A}\\
\cat{T} \ar[r]_-{\theta} & {!} \den{A} \ar[u]_-{d}
}
\end{equation}
commutes. The space of tangent vectors at $\pi$ is denoted $T_{\pi}$.
\end{definition}

It follows from Example \ref{example:tangent_coalgebra} that there is a linear isomorphism
\[
\den{A} \lto T_{\pi}
\]
sending $Q \in \den{A}$ to the coalgebra morphism $\theta$ with $\theta(1) = \ket{o}_{\den{\pi}}$ and $\theta(\varepsilon) = \ket{Q}_{\den{\pi}}$. 

Note that the denotation of a program not only maps inputs to outputs (if we identify inputs and outputs with vacuum vectors) but also tangent vectors to tangent vectors. To wit, if $\rho$ is a proof of a sequent $!A \vdash B$ with denotation $\lambda: {!} \den{A} \lto \den{B}$, then composing a tangent vector $\theta$ at a proof $\pi$ of $\vdash A$ with the lifting $\Lambda$ of $\lambda$ leads to a tangent vector at the cut of $\rho$ against the promotion of $\pi$. That is, the linear map
\begin{equation}\label{eq:fake_tangent_map}
\xymatrix@C+2pc{
\cat{T} \ar[r]^-{\theta} & ! \den{A} \ar[r]^-{\Lambda} & ! \den{B}
}
\end{equation}
is a tangent vector at the following proof, which we denote $\rho \l \pi$
\begin{prooftree}
\AxiomC{$\pi$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$\vdash A$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize prom}
\UnaryInfC{$\vdash {!} A$}
\AxiomC{$\rho$}
\noLine\UnaryInfC{$\vdots$}
\def\extraVskip{5pt}
\noLine\UnaryInfC{$!A \vdash B$}
\def\extraVskip{2pt}
\RightLabel{\scriptsize cut}
\BinaryInfC{$\vdash B$}
\end{prooftree}
By Theorem \ref{theorem:describe_lifting} the linear map of tangent spaces induced in this way by $\rho$ is
\begin{gather}
\den{A} \cong T_{\pi} \lto T_{\rho\l\pi} \cong \den{B}\label{eq:tangent_map_logic}\\
Q \longmapsto \lambda \ket{Q}_{\den{\pi}}\nonumber
\end{gather}

When $\rho$ computes a smooth map of differentiable manifolds, this map can be compared with an actual map of tangent spaces. We examine $\rho = \church{2}_A$ below. It would be interesting to understand these maps in more complicated examples; this seems to be related to the differential $\lambda$-calculus \cite{ehrhard_difflambda,ehrhard_difflambda2}, but we have not tried to work out the precise connection.

\begin{example}\label{example:tangent_to_2} When $k = \mathbb{C}$ and $Z = \den{\church{2}_A}_{nl}$ we have by Lemma \ref{lemma:nonlinear_recover}
\[
Z: M_n(\mathbb{C}) \lto M_n(\mathbb{C}), \qquad Z(\alpha) = \alpha^2\,.
\]
The tangent map of the smooth map of manifolds $Z$ at $\alpha$ is $(Z_*)_\alpha( \nu ) = \{ \nu, \alpha \}$. When $\alpha$ is the denotation of some proof $\pi$ of $\vdash A \multimap A$ this agrees with the tangent map assigned in \eqref{eq:tangent_map_logic} to the proof $\church{2}_A$ at $\pi$, using \eqref{eq:church_2_den}.
\end{example}

\section{Translation}\label{remark:embedding_intuit}

We have already mentioned the equivalence of the simply-typed $\lambda$-calculus and propositional intuitionistic logic under the Curry-Howard isomorphism \cite[\S 6.5]{selinger} (TODO no we haven't). There is an embedding of propositional intuitionistic logic into propositional intuitionistic \emph{linear} logic \cite[\S 5.1]{girard_llogic} making use of the additive connectives of linear logic which we have omitted in the above. This means that every program in the simply-typed $\lambda$-calculus may be assigned a proof in linear logic (with additives) in such a way that $\beta$-reduction in the $\lambda$-calculus corresponds to cut-elimination in linear logic. For more on linear logic proofs as computer programs, see \cite{lafont,abramsky,benton_etal}.

For example, if $A,B$ denote types in propositional intuitionistic logic, and $A^\circ, B^\circ$ the corresponding types in linear logic, then $(A \Rightarrow B)^\circ := (! A^\circ) \multimap B^\circ$ where for atoms $A$ we declare $A^\circ = A$. There is a corresponding translation of proofs of $\vdash A$ to proofs of $\vdash A^\circ$. The Church numeral $\church{2}_A$ is a $\lambda$-term of type $(A \rightarrow A) \rightarrow (A \rightarrow A)$ which corresponds to a proof in intuitionistic logic of the sequent $\vdash (A \Rightarrow A) \Rightarrow (A \Rightarrow A)$. If $A$ is atomic, the translation of this type to linear logic is $\inta_A' = {!}( {!}A \multimap A ) \multimap ({!}A \multimap A)$. Thus a Church numeral in the $\lambda$-calculus determines a proof of $\vdash \inta_A'$ not $\vdash \inta_A$ under this translation. However, this translation is not necessarily the most useful or economical one because of the over-use of exponentials \cite[\S 5.3]{girard_llogic}. For reasons of clarity we follow standard practice in using the ``linear logic version'' of the Church numeral $\church{2}_A$ in Example \ref{example:first_occur_2} above, rather than the literal translation.

\bibliographystyle{amsalpha}
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\href}[2]{#2}
\begin{thebibliography}{BHLS03}

\bibitem{abramsky}
S.~Abramsky, \textsl{Computational interpretations of linear logic}, Theoretical Computer Science, 1993.

\bibitem{abramsky4}
S.~Abramsky, \textsl{Retracting some paths in process algebra}, In CONCUR 96, Springer Lecture Notes in Computer Science \textbf{1119}, 1--17, 1996.

\bibitem{abramsky2}
S.~Abramsky, \textsl{Geometry of {I}nteraction and linear combinatory algebras}, Mathematical Structures in Computer Science, \textbf{12}, 625--665, 2002.

\bibitem{abramsky3}
S.~Abramsky and R.~Jagadeesan, \textsl{New foundations for the {G}eometry of {I}nteraction}, Information and Computation \textbf{111} (1), 53--119, 1994.

\bibitem{anel}
M.~Anel, A.~Joyal, \textsl{Sweedler theory of (co)algebras and the bar-cobar constructions}, \href{http://arxiv.org/abs/1309.6952}{[arXiv:1309.6952]}

\bibitem{atiyah}
M.~Atiyah, \textsl{Topological quantum field theories}, Publications Math\'{e}matique de l'IH\'{E}S 68, 175--186, 1989.

\bibitem{baez}
J.~Baez and M.~Stay, \textsl{Physics, topology, logic and computation: a Rosetta stone}, in B. Coecke (ed.) New Structures for Physics, Lecture Notes in Physics 813, Springer, Berlin, 95--174, 2011

\bibitem{barr}
M.~Barr, \textsl{Coalgebras over a commutative ring}, Journal of Algebra 32, 600--610, 1974.

\bibitem{barr_auto}
\bysame, \textsl{$\star$-autonomous categories}, Number 752 in Lecture Notes in Mathematics. Springer-Verlag, 1979.

\bibitem{barr_acc}
\bysame, \textsl{Accessible categories and models of linear logic}, Journal of Pure and Applied Algebra, 69(3):219--232, 1990.

\bibitem{barr_autolin}
\bysame, {$?$-autonomous categories and linear logic}, Mathematical Structures in Computer Science, 1(2):159--178, 1991.

\bibitem{barr_chu}
\bysame, \textsl{The {C}hu construction: history of an idea}, Theory and Applications of Categories, Vol. 17, No. 1, 10--16, 2006.

\bibitem{benton}
N.~Benton, \textsl{A mixed linear and non-linear logic; proofs, terms and models}, in Proceedings of Computer Science Logic 94, vol. 933 of Lecture Notes in Computer Science, Verlag, 1995.

\bibitem{benton_etal}
N.~Benton, G.~Bierman, V.~de Paiva and M.~Hyland, \textsl{Term assignment for intuitionistic linear logic}, Technical report 262, Computer Laboratory, University of Cambridge, 1992.

\bibitem{block-leroux} 
R.~Block, P.~Leroux, \textsl{Generalized dual coalgebras of algebras, with applications to cofree coalgebras}, J. Pure Appl. Algebra 36, no. 1, 15--21, 1985.

\bibitem{blute}
R.~Blute, \textsl{Hopf algebras and linear logic}, Mathematical Structures in Computer Science, 6(2):189--217, 1996.

\bibitem{blute_scott}
R.~Blute and P.~Scott, \textsl{Linear {L}a\"{u}chli semantics}, Annals of Pure and Applied Logic, 77:101--142, 1996.

\bibitem{blue_book}
\bysame, \textsl{Category theory for linear logicians}, Linear Logic in Computer Science 316: 3--65, 2004.

\bibitem{blute_fock}
R.~Blute, P.~Panangaden, R.~Seely, \textsl{Fock space: a model of linear exponential types}, in: Proc. Ninth Conf. on Mathematical Foundations of Programming Semantics, Lecture Notes in Computer Science, Vol. 802, Springer, Berlin, 1--25, 1994.

\bibitem{bott}
R.~Bott and L.W.~Tu, \textsl{Differential forms in {A}lgebraic {T}opology}, Graduate Texts in Mathematics, \textbf{82}, Springer, 1982.

\bibitem{brauner}
T.~Brauner, \textsl{Introduction to linear logic}, Basic Research in Computer Science, Lecture Notes (1996).

\bibitem{ct1007.2679}
A.~{C\u ald\u araru} and S.~Willerton, \textsl{The Mukai pairing, I: a categorical approach},
New York Journal of Mathematics \textbf{16}, 61--98, 2010
  \href{http://arxiv.org/abs/0707.2052}{[arXiv:0707.2052]}.
  
\bibitem{lgdual}
N.~Carqueville and D.~Murfet, \textsl{Adjunctions and defects in {L}andau-{G}inzburg models}, \href{http://arxiv.org/abs/1208.1481}{[arXiv:1208.1481]}.

\bibitem{cr0909.4381}
N.~Carqueville and I.~Runkel, \textsl{On the monoidal structure of matrix bi-factorisations}, J. Phys.
  A: Math. Theor. \textbf{43} 275--401, 2010
  \href{http://arxiv.org/abs/0909.4381}{[arXiv:0909.4381]}.

\bibitem{church}
A.~Church, \textsl{The {C}alculi of {L}ambda-conversion}, Princeton University Press, Princeton, N. J. 1941.

\bibitem{danos}
V.~Danos and J.-B.~Joinet, \textsl{Linear logic and elementary time}, Information and Computation 183, 123--127, 2003.

\bibitem{danos_regnier1}
V.~Danos and L.~Regnier, \textsl{Local and {A}synchronous beta-reduction (an analysis of {G}irard's execution formula)} in: Springer Lecture Notes in Computer Science \textbf{8}, 296--306, 1993.

\bibitem{danos_regnier2}
V.~Danos and L.~Regnier, \textsl{Proof-nets and the Hilbert space}, in (Girard \textsl{et. al.} 1995), 307--328, 1995.

\bibitem{denning}
P.~J.~Denning, \textsl{Ubiquity symposium ``What is computation?''}: opening statement, Ubiquity 2010. Available on the \href{http://ubiquity.acm.org/article.cfm?id=1870596}{Ubiquity website}.

\bibitem{dm1102.2957}
T.~Dyckerhoff and D.~Murfet, \textsl{Pushing forward matrix factorisations}, Duke Math. J. Volume 162, Number 7 1249--1311, 2013 \href{http://arxiv.org/abs/1102.2957}{[arXiv:1102.2957]}.

\bibitem{ehrhard}
T.~Ehrhard, \textsl{Finiteness spaces}, Math. Structures Comput. Sci. 15 (4) 615--646, 2005.

\bibitem{ehrhard_kothe}
\bysame, \textsl{On {K}\"othe sequence spaces and linear logic}, Mathematical Structures in Computer Science 12.05, 579--623, 2002.

\bibitem{ehrhard_difflambda}
T.~Ehrhard and L.~Regnier, \textsl{The differential lambda-calculus}, Theoretical Computer Science 309.1: 1--41, 2003.

\bibitem{ehrhard_difflambda2}
\bysame, \textsl{Differential interaction nets}, Theoretical Computer Science 364.2: 166--195, 2006.

\bibitem{gentzen}
G.~Gentzen, \textsl{The Collected Papers of Gerhard Gentzen}, (Ed. M. E. Szabo), Amsterdam, Netherlands: North-Holland, 1969.

\bibitem{getzler}
E.~Getzler, P.~Goerss, \emph{A model category structure for differential graded coalgebras}, preprint, 1999.
  
\bibitem{girard_llogic}
J.-Y.~Girard, \textsl{Linear Logic}, Theoretical Computer Science 50 (1), 1--102, 1987.

\bibitem{girard_normal}
\bysame, \textsl{Normal functors, power series and the $\lambda$-calculus} Annals of Pure and Applied
Logic, 37: 129--177, 1988.

\bibitem{girard_goi1}
\bysame, \textsl{Geometry of {I}nteraction I: {I}interpretation of {S}ystem {F}}, in Logic Colloquium '88, ed. R.~Ferro, et al. North-Holland, 221--260, 1988.

\bibitem{girard_goi2}
\bysame, \textsl{Geometry of {I}nteraction II: {D}eadlock-free {A}lgorithms}, COLOG-88, Springer Lecture Notes in Computer Science \textbf{417}, 76--93, 1988.

\bibitem{girard_goi3}
\bysame, \textsl{Geometry of {I}nteraction III: {A}ccommodating the {A}dditives}, in (Girard \textsl{et al}. 1995), pp.1--42.

\bibitem{girard_towards}
\bysame, \textsl{Towards a geometry of interaction}, In J.~W.~Gray and A.~Scedrov, editors, Categories in Computer Science and Logic, volume 92 of Contemporary Mathematics, 69--108, AMS, 1989.

\bibitem{girard_complexity}
\bysame, \textsl{Light linear logic}, Information and Computation 14, 1995.

\bibitem{girard_coherentbanach}
\bysame, \textsl{Coherent {B}anach spaces: a continuous denotational semantics}, Theoretical Computer Science, 227: 275--297, 1999.

\bibitem{girard_blindspot}
\bysame, \textsl{The Blind Spot: lectures on logic}, European Mathematical Society, 2011.

\bibitem{girard_prooftypes}
J.-Y.~Girard, Y.~Lafont, and P.~Taylor, \textsl{Proofs and Types}, Cambridge Tracts in Theoretical Computer Science 7 ,Cambridge University Press, 1989.

\bibitem{Gonthier}
G.~Gontheir, M.~Abadi and J.-J.~L\'{e}vy, \textsl{The geometry of optimal lambda reduction}, in 9th Annual IEEE Symp. on Logic in Computer Science (LICS), 15--26, 1992.
  
\bibitem{haghverdi}
E.~Haghverdi and P.~Scott, \textsl{Geometry of {I}nteraction and the dynamics of prood reduction: a tutorial}, in New Structures for Physics, Lecture notes in Physics \textbf{813}, 357--417, 2011.
  
\bibitem{hazewinkel}
H.~Hazewinkel, \textsl{Cofree coalgebras and multivariable recursiveness}, J. Pure Appl. Algebra 183, no. 1--3, 61--103, 2003.

\bibitem{hyland}
M.~Hyland and A.~Schalk, \textsl{Glueing and orthogonality for models of linear logic}, Theoretical Computer Science, 294: 183--231, 2003.

\bibitem{JSGoTCI}
A.~Joyal and R.~Street, \textsl{The geometry of tensor calculus I}, Advances in Math. \textbf{88}, 55--112, 1991.

\bibitem{JSGoTCII}
A.~Joyal and R.~Street, \textsl{The geometry of tensor calculus II}, 
draft available at 
\href{http://maths.mq.edu.au/~street/GTCII.pdf}{http://maths.mq.edu.au/\textasciitilde street/GTCII.pdf}

\bibitem{joyal_trace}
A.~Joyal, R.~Street and D.~Verity, \textsl{Traced monoidal categories}, Math. Proc. Camb. Phil. Soc. 119, 447--468, 1996.

\bibitem{khovdia}
M.~Khovanov, \textsl{Categorifications from planar diagrammatics}, Japanese J. of Mathematics \textbf{5}, 153--181, 2010 \href{http://arxiv.org/abs/1008.5084}{[arXiv:1008.5084]}.
  
\bibitem{lafont}
Y.~Lafont, \textsl{The {L}inear {A}bstract {M}achine}, Theoretical Computer Science, 59 (1,2):157--180, 1988.

\bibitem{lambek}
J.~Lambek and P.~J.~Scott, \textsl{Introduction to higher order categorical logic}, Cambridge Studies in Advanced Mathematics, vol. 7, Cambridge University Press, Cambridge, 1986.

\bibitem{ladia}
A.~D.~Lauda, \textsl{An introduction to diagrammatic algebra and categorified quantum $\mathfrak{sl}_2$}, Bulletin of the Institute of Mathematics Academia Sinica (New Series), Vol. \textbf{7}, No. 2, 165--270, 2012 \href{http://arxiv.org/abs/1106.2128}{[arXiv:1106.2128]}.

\bibitem{mccarthy}
J.~McCarthy, \textsl{Recursive functions of symbolic expressions and their computation by machine, Part I.}, Communications of the ACM 3.4: 184--195, 1960.

\bibitem{McNameethesis}
D.~McNamee, \textsl{On the mathematical structure of topological defects in
  {L}andau-{G}inzburg models}, MSc Thesis, Trinity College Dublin, 2009.
  
\bibitem{mellies_dia}
P.-A.~Melli\`{e}s, \textsl{Functorial boxes in string diagrams}, In Z. \'{E}sik, editor, Computer Science Logic,
volume 4207 of Lecture Notes in Computer Science, pages 1--30, Springer Berlin / Heidelberg,
2006.

\bibitem{mellies}
P-A.~Melli\`{e}s, \textsl{Categorical semantics of linear logic}, in : Interactive models of computation and program behaviour, Panoramas et Synth\`{e}ses $27$, Soci\'{e}t\'{e} Math\'{e}matique de France, 2009.

\bibitem{mellies2}
P.-A.~Melli\`{e}s, N.~Tabareau, C.~Tasson, \textsl{An explicit formula for the free exponential modality of linear logic}, in: 36th International Colloquium on Automata, Languages and Programming, July 2009, Rhodes, Greece, 2009.

\bibitem{murfet}
D.~Murfet, \textsl{Computing with cut systems}, \href{http://arxiv.org/abs/1402.4541}{[arXiv:1402.4541]}.

\bibitem{murfet_coalg}
D.~Murfet, \textsl{On Sweedler's cofree cocommutative coalgebra}, \href{http://arxiv.org/abs/1406.5749}{[arXiv:1406.5749]}.

\bibitem{pagani}
M.~ Pagani and L.~Tortora de Falco, \textsl{Strong normalization property for second order linear logic}, Theoretical Computer Science 411.2 (2010): 410--444.

\bibitem{pv}
A.~Polishchuk and A.~Vaintrob, \textsl{Chern characters and {H}irzebruch-{R}iemann-{R}och formula for matrix factorizations}, Duke Mathematical Journal 161.10: 1863--1926, 2012 \href{http://arxiv.org/abs/1002.2116}{[arXiv:1002.2116]}. 

\bibitem{schreiber}
U.~Schreiber, \textsl{Quantization via {L}inear homotopy types}, \href{http://arxiv.org/abs/1402.7041}{[arXiv:1402.7041]}.

\bibitem{scott}
D.~Scott, \textsl{Data types as lattices}, SIAM Journal of computing, 5:522--587, 1976.

\bibitem{scott_talk}
\bysame, \textsl{The {L}ambda calculus, then and now}, available on \href{http://www.youtube.com/watch?v=7cPtCpyBPNI}{YouTube} with \href{http://turing100.acm.org/lambda_calculus_timeline.pdf}{lecture notes}, 2012.

\bibitem{seely}
R.~Seely, \textsl{Linear logic, star-autonomous categories and cofree coalgebras}, Applications of categories in logic and computer science, Contemporary Mathematics, 92, 1989.

\bibitem{selinger}
P.~Selinger, \textsl{Lecture notes on the {L}ambda calculus}, \href{http://arxiv.org/abs/0804.3434}{[arXiv:0804.3434]}.

\bibitem{shirahata}
M.~Shirahata, \textsl{Geometry of {I}nteraction explained}, available \href{http://www.kurims.kyoto-u.ac.jp/~hassei/algi-13/kokyuroku/19_shirahata.pdf}{online}.

\bibitem{soare}
R.~I.~Soare, \textsl{Computability and {I}ncomputability}, in CiE 2007: Computation and Logic in the Real World, LNCS 4497, Springer, 705--715.
  
\bibitem{sweedler}
M.~Sweedler, \textsl{Hopf Algebras}, W.~A.~Benjamin, New York, 1969.

\bibitem{valiron}
B.~Valiron and S.~Zdancewic, \textsl{Finite vector spaces as model of simply-typed lambda-calculi}, \href{http://arxiv.org/abs/1406.1310v1}{[arXiv:1406.1310]}.

\bibitem{quillen}
D.~Quillen, \textsl{Rational homotopy theory}, The Annals of Mathematics, Second Series, Vol. 90, No. 2, 205--295, 1969.

\bibitem{univalent}
The Univalent Foundations Program, \textsl{Homotopy {T}ype {T}heory: {U}nivalent {F}oundations of {M}athematics}, Institute for Advanced Study (Princeton), 2013.

\bibitem{Yoshino90}
Y.~Yoshino, \emph{Cohen-{M}acaulay modules over {C}ohen-{M}acaulay rings},
  London Mathematical Society Lecture Note Series, vol. 146, Cambridge
  University Press, Cambridge, 1990. 
  
\bibitem{witten}
E.~Witten, \textsl{Topological quantum field theory}, \textsl{Communications in Mathematical Physics}, 117 (3), 353--386, 1988.

\end{thebibliography}

\end{document}

\section{Cuts}

\begin{example} For any type $A$, addition of two $A$-integers is represented by the following cut-free proof $\prf{\mathrm{add}}$ of $\inta_A, \inta_A \vdash \inta_A$
%\scalebox{0.6}{
%\parbox{2cm}{
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$!(A \multimap A) \vdash {!}(A \multimap A)$}
\AxiomC{}
\UnaryInfC{$!(A \multimap A) \vdash {!}(A \multimap A)$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\AxiomC{}
\UnaryInfC{$A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$A, A \multimap A, A \multimap A \vdash A$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$A \multimap A, A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$!(A \multimap A), \inta_A, A \multimap A \vdash A \multimap A$}
\RightLabel{\scriptsize$\multimap L$}
\BinaryInfC{$!(A \multimap A), \inta_A, \inta_A \vdash A \multimap A$}
\RightLabel{\scriptsize$\multimap R$}
\UnaryInfC{$\inta_A, \inta_A \vdash \inta_A$}
\end{prooftree}
The denotation of which is a morphism
\[
\den{\prf{\mathrm{add}}}: \den{\inta_A} \otimes \den{\inta_A} \lto \den{\inta_A}
\]
To actually perform an addition, we would \emph{cut} two Church numerals against this proof, for instance with the following proof tree:
\[
blah
\]
The denotation of this combined proof is the composite
\[
\den{\prf{\mathrm{add}}} \circ ( \den{\church{m}} \otimes \den{\church{n}} ): k \lto \den{\inta_A}\,.
\]
The cut-elimination process gives us a deterministic algorithm for replacing this proof by a cut-free proof of the same formula $\inta_A$. It is a worthwhile exercise to perform this cut-elimination and verify that the resulting cut-free proof is precisely the integer $\church{n+m}$. Alternatively one can prove that the Church numerals are the \emph{only} cut-free proofs of $\inta_A$ and then use the semantics to distinguish between them.
\end{example}

% TIKZ ---- picture of mult_2
\begin{tikzpicture}[scale=0.35,auto,inner sep=1mm]
\coordinate (topr) at (0,2);
\coordinate (comp) at (3,4);
\drawbang[out=90,in=180] (topr) to (comp);
\drawprom (0,-1) ellipse (3cm and 3cm);
\dernode (bottomr) at (0,-4) {};

% -------- promoted 2 ---------
% All coordinates are relative to the composition vertex (o), which itself
% has its coordinates derived from topr. This is the only link to the outside namespace.
\coordinate (o) at ($ (topr) + (1,-2) $);
\coordinate (ellipse_center) at ($ (o) - (1,1) $);
\coordinate (elbow) at ($ (o) - (3, 0.5) $);
\coordinate (top_of_circle) at ($ (o) + (-1,2) $);
\draw[out=90,in=270] (o) to (top_of_circle);
\drawprom (ellipse_center) ellipse (3cm and 3cm); % outer boundary
\dernode (bottomr) at ($ (o) - (1,4) $) {};
\dernode (R) at ($ (o) + (1, -1.5) $) {}; % right dereliction
\dernode (L) at ($ (o) - (1,1.5) $) {}; % left dereliction
\coordinate (left_curve) at ($ (o) - (2.5,2) $);
\coordinate (left_meet) at ($ (o) - (1.5, 0.9) $);
\coordinate (delta) at ($ (o) - (0, 2.7) $); % coproduct vertex
\draw[out=90,in=0] (R) to node [swap] {} (o);
\drawbang[out=0,in=270] (delta) to (R);
\drawbang[out=180,in=270] (delta) to (L);
\drawbang[out=90,in=270] (bottomr) to (delta);
\draw[out=90,in=180] (left_meet) to (o);
\draw[out=90,in=0] (L) to (left_meet);
\draw[out=0,in=180] (left_curve) to (left_meet);
\draw[out=180,in=270] (left_curve) to (elbow);
\draw[out=90,in=180] (elbow) to ($ (o)!.5!(top_of_circle) $);
% ------ end promoted 2 ------

% Now for the rest of the diagram
\coordinate (curve) at (-3.9, -3);
\coordinate (meet) at (3,5.5);
\drawbang[out=300,in=270] (curve) to (bottomr);
\drawbang[out=120,in=180] (curve) to (meet);
\node (inta) at (6.5,-5) {$\inta_A$};
\draw[out=90,in=0] (inta) to (comp);
\node (vtop) at (3,7) {$\inta_A$};
\draw (meet) to (vtop);
\draw (comp) to (meet);
\end{tikzpicture}

&

\subsection{A sketch of future work}\label{section:sketch}

%However the connection we want to emphasise, and which will be elaborated further in \cite{??}, is between linear logic and algebraic geometry and higher categories. 

Proofs in linear logic naturally have shadows in contexts where there is an interaction between \emph{linear} objects and the \emph{non-linear} objects which classify them (e.g. vector bundles and moduli spaces of vector bundles). We have in mind the following kind of situation: let $\cat{B}$ be a bicategory which we think of as having algebraic spaces as objects and some kind of correspondences or integral transforms as $1$-morphisms. Two of the assumptions we make about this bicategory are:
\begin{itemize}
\item[(i)] It is symmetric monoidal and closed, so that for a pair of objects $v,w$ we may form $v \otimes w$ and $\Hom(v,w)$, and
\item[(ii)] for each pair of objects $v,w$ there is a reasonable moduli space $\mathscr{M}_{v,w}$ whose points are in bijection with $1$-morphisms $v \lto w$ in $\cat{B}$ and that these moduli spaces themselves form valid objects of $\cat{B}$.
\end{itemize}
What distinguishes $\mathscr{M}_{v,w}$ from $\Hom(v,w)$ is that the former is supposed to be ``space-like'' which means it is a coalgebra, i.e. comes equipped with natural $1$-morphisms $\mathscr{M}_{v,w} \lto \mathscr{M}_{v,w} \otimes \mathscr{M}_{v,w}$ and $\mathscr{M}_{v,w} \lto 1$. In for example the bicategory of smooth projective varieties every object is equipped with such $1$-morphisms, given by the diagonal and the graph of the structure map.

It therefore makes sense to ask for a ``universal'' $1$-morphism $\mathscr{M}_{v,w} \lto \Hom(v,w)$ whose value over a point $[X]$ of the moduli space corresponding to $X: v \lto w$ is $X$ itself.

In this situation we can expect to find a semantics of linear logic in the bicategory $\cat{B}$. For instance, if the formula $A$ of the logic is assigned the object $v$ of the bicategory, then the proof \eqref{church_2_intro} given above is represented by a $1$-morphism
\[
\mathscr{M}_{v,v} \lto \Hom(v,v)
\]
whose value over a point $[X]$ of $\mathscr{M}_{v,v}$ is the square $X \circ X$ of a loop $X: v \lto v$. In this way arbitrary integers, encoded as proofs in linear logic, may be represented as $1$-morphisms in $\cat{B}$, and the same is true any program computing integers from integers that can be encoded in linear logic. The remarkable thing is that all of this structure is immediately present as soon as we allow the objects $\Hom(v,w)$ to interact with their own moduli spaces. In \cite{??} and its forthcoming sequels this is made precise when $\cat{B}$ is a bicategory of matrix factorisations.

% What happens when we stabilise by ensuring that between any two objects there is always a generic? In a symmetric monoidal category we know how to enumerate all the "canonical" morphisms: they are terms in some language. What is the language that tells us how to enumerate all the canonical constructions that are possible in a setting such as the one we have just enumerated? That is, the structures that do not depend on the particular properties of matrix factorisations and are ``inherent'' in this notion of taking iterated moduli spaces? This language is linear logic.

% One should not underestimate the rapid progress in abstraction and uptake of academic ideas between computer science and industry. Maybe in a few decades it will be ordinary for Google employees to be programming with simplicial sets and Kan fibrations \cite{??}.
